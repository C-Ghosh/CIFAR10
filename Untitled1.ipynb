{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "X_XAyeFT_i8u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7be0c8c6-2fb4-4390-faa1-6a113c13f7a3"
      },
      "cell_type": "code",
      "source": [
        "! wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-10-28 14:51:26--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  3.92MB/s    in 43s     \n",
            "\n",
            "2018-10-28 14:52:10 (3.75 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1Urc8aiwM1q_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "3da503ff-4551-40b9-a11f-1ff50f8ed01d"
      },
      "cell_type": "code",
      "source": [
        "!tar -xvf cifar-10-python.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Us5uAf17NAEI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cifar10_dataset_folder_path = \"cifar-10-batches-py/\" "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VJ8b2T5fOcQp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Helper Functions"
      ]
    },
    {
      "metadata": {
        "id": "U4x2R8DmNFFp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "\n",
        "def _load_label_names():\n",
        "    \"\"\"\n",
        "    Load the label names from file\n",
        "    \"\"\"\n",
        "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "\n",
        "def load_cfar10_batch(cifar10_dataset_folder_path, batch_id):\n",
        "    \"\"\"\n",
        "    Load a batch of the dataset\n",
        "    \"\"\"\n",
        "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
        "        batch = pickle.load(file, encoding='latin1')\n",
        "\n",
        "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
        "    labels = batch['labels']\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "def display_stats(cifar10_dataset_folder_path, batch_id, sample_id):\n",
        "    \"\"\"\n",
        "    Display Stats of the the dataset\n",
        "    \"\"\"\n",
        "    batch_ids = list(range(1, 6))\n",
        "\n",
        "    if batch_id not in batch_ids:\n",
        "        print('Batch Id out of Range. Possible Batch Ids: {}'.format(batch_ids))\n",
        "        return None\n",
        "\n",
        "    features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_id)\n",
        "\n",
        "    if not (0 <= sample_id < len(features)):\n",
        "        print('{} samples in batch {}.  {} is out of range.'.format(len(features), batch_id, sample_id))\n",
        "        return None\n",
        "\n",
        "    print('\\nStats of batch {}:'.format(batch_id))\n",
        "    print('Samples: {}'.format(len(features)))\n",
        "    print('Label Counts: {}'.format(dict(zip(*np.unique(labels, return_counts=True)))))\n",
        "    print('First 20 Labels: {}'.format(labels[:20]))\n",
        "\n",
        "    sample_image = features[sample_id]\n",
        "    sample_label = labels[sample_id]\n",
        "    label_names = _load_label_names()\n",
        "\n",
        "    print('\\nExample of Image {}:'.format(sample_id))\n",
        "    print('Image - Min Value: {} Max Value: {}'.format(sample_image.min(), sample_image.max()))\n",
        "    print('Image - Shape: {}'.format(sample_image.shape))\n",
        "    print('Label - Label Id: {} Name: {}'.format(sample_label, label_names[sample_label]))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(sample_image)\n",
        "\n",
        "\n",
        "def _preprocess_and_save(normalize, one_hot_encode, features, labels, filename):\n",
        "    \"\"\"\n",
        "    Preprocess data and save it to file\n",
        "    \"\"\"\n",
        "    features = normalize(features)\n",
        "    labels = one_hot_encode(labels)\n",
        "\n",
        "    pickle.dump((features, labels), open(filename, 'wb'))\n",
        "\n",
        "\n",
        "def preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode):\n",
        "    \"\"\"\n",
        "    Preprocess Training and Validation Data\n",
        "    \"\"\"\n",
        "    n_batches = 5\n",
        "    valid_features = []\n",
        "    valid_labels = []\n",
        "\n",
        "    for batch_i in range(1, n_batches + 1):\n",
        "        features, labels = load_cfar10_batch(cifar10_dataset_folder_path, batch_i)\n",
        "        validation_count = int(len(features) * 0.1)\n",
        "\n",
        "        # Prprocess and save a batch of training data\n",
        "        _preprocess_and_save(\n",
        "            normalize,\n",
        "            one_hot_encode,\n",
        "            features[:-validation_count],\n",
        "            labels[:-validation_count],\n",
        "            'preprocess_batch_' + str(batch_i) + '.p')\n",
        "\n",
        "        # Use a portion of training batch for validation\n",
        "        valid_features.extend(features[-validation_count:])\n",
        "        valid_labels.extend(labels[-validation_count:])\n",
        "\n",
        "    # Preprocess and Save all validation data\n",
        "    _preprocess_and_save(\n",
        "        normalize,\n",
        "        one_hot_encode,\n",
        "        np.array(valid_features),\n",
        "        np.array(valid_labels),\n",
        "        'preprocess_validation.p')\n",
        "\n",
        "    with open(cifar10_dataset_folder_path + '/test_batch', mode='rb') as file:\n",
        "        batch = pickle.load(file, encoding='latin1')\n",
        "\n",
        "    # load the training data\n",
        "    test_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
        "    test_labels = batch['labels']\n",
        "\n",
        "    # Preprocess and Save all training data\n",
        "    _preprocess_and_save(\n",
        "        normalize,\n",
        "        one_hot_encode,\n",
        "        np.array(test_features),\n",
        "        np.array(test_labels),\n",
        "        'preprocess_training.p')\n",
        "\n",
        "\n",
        "def batch_features_labels(features, labels, batch_size):\n",
        "    \"\"\"\n",
        "    Split features and labels into batches\n",
        "    \"\"\"\n",
        "    for start in range(0, len(features), batch_size):\n",
        "        end = min(start + batch_size, len(features))\n",
        "        yield features[start:end], labels[start:end]\n",
        "\n",
        "\n",
        "def load_preprocess_training_batch(batch_id, batch_size):\n",
        "    \"\"\"\n",
        "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
        "    \"\"\"\n",
        "    filename = 'preprocess_batch_' + str(batch_id) + '.p'\n",
        "    features, labels = pickle.load(open(filename, mode='rb'))\n",
        "\n",
        "    # Return the training data in batches of size <batch_size> or less\n",
        "    return batch_features_labels(features, labels, batch_size)\n",
        "\n",
        "\n",
        "def display_image_predictions(features, labels, predictions):\n",
        "    n_classes = 10\n",
        "    label_names = _load_label_names()\n",
        "    label_binarizer = LabelBinarizer()\n",
        "    label_binarizer.fit(range(n_classes))\n",
        "    label_ids = label_binarizer.inverse_transform(np.array(labels))\n",
        "\n",
        "    fig, axies = plt.subplots(nrows=4, ncols=2)\n",
        "    fig.tight_layout()\n",
        "    fig.suptitle('Softmax Predictions', fontsize=20, y=1.1)\n",
        "\n",
        "    n_predictions = 3\n",
        "    margin = 0.05\n",
        "    ind = np.arange(n_predictions)\n",
        "    width = (1. - 2. * margin) / n_predictions\n",
        "\n",
        "    for image_i, (feature, label_id, pred_indicies, pred_values) in enumerate(zip(features, label_ids, predictions.indices, predictions.values)):\n",
        "        pred_names = [label_names[pred_i] for pred_i in pred_indicies]\n",
        "        correct_name = label_names[label_id]\n",
        "\n",
        "        axies[image_i][0].imshow(feature)\n",
        "        axies[image_i][0].set_title(correct_name)\n",
        "        axies[image_i][0].set_axis_off()\n",
        "\n",
        "        axies[image_i][1].barh(ind + margin, pred_values[::-1], width)\n",
        "        axies[image_i][1].set_yticks(ind + margin)\n",
        "        axies[image_i][1].set_yticklabels(pred_names[::-1])\n",
        "        axies[image_i][1].set_xticks([0, 0.5, 1.0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vjD5cX1WKhJv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Explore the Data"
      ]
    },
    {
      "metadata": {
        "id": "n_H4U_v7NbF7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "outputId": "3ac6d06d-c437-4148-d3f8-271df0350eae"
      },
      "cell_type": "code",
      "source": [
        "batch_id = 1\n",
        "sample_id = 17\n",
        "display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Stats of batch 1:\n",
            "Samples: 10000\n",
            "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
            "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
            "\n",
            "Example of Image 17:\n",
            "Image - Min Value: 25 Max Value: 225\n",
            "Image - Shape: (32, 32, 3)\n",
            "Label - Label Id: 3 Name: cat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f9883c070f0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "5BuVMdjTOjVp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Testing Functions"
      ]
    },
    {
      "metadata": {
        "id": "fcp6rdyoOTqm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from unittest.mock import MagicMock\n",
        "\n",
        "\n",
        "def _print_success_message():\n",
        "    print('Tests Passed')\n",
        "\n",
        "\n",
        "def test_folder_path(cifar10_dataset_folder_path):\n",
        "    assert cifar10_dataset_folder_path is not None,\\\n",
        "        'Cifar-10 data folder not set.'\n",
        "    assert cifar10_dataset_folder_path[-1] != '/',\\\n",
        "        'The \"/\" shouldn\\'t be added to the end of the path.'\n",
        "    assert os.path.exists(cifar10_dataset_folder_path),\\\n",
        "        'Path not found.'\n",
        "    assert os.path.isdir(cifar10_dataset_folder_path),\\\n",
        "        '{} is not a folder.'.format(os.path.basename(cifar10_dataset_folder_path))\n",
        "\n",
        "    train_files = [cifar10_dataset_folder_path + '/data_batch_' + str(batch_id) for batch_id in range(1, 6)]\n",
        "    other_files = [cifar10_dataset_folder_path + '/batches.meta', cifar10_dataset_folder_path + '/test_batch']\n",
        "    missing_files = [path for path in train_files + other_files if not os.path.exists(path)]\n",
        "\n",
        "    assert not missing_files,\\\n",
        "        'Missing files in directory: {}'.format(missing_files)\n",
        "\n",
        "    print('All files found!')\n",
        "\n",
        "\n",
        "def test_normalize(normalize):\n",
        "    test_shape = (np.random.choice(range(1000)), 32, 32, 3)\n",
        "    test_numbers = np.random.choice(range(256), test_shape)\n",
        "    normalize_out = normalize(test_numbers)\n",
        "\n",
        "    assert type(normalize_out).__module__ == np.__name__,\\\n",
        "        'Not Numpy Object'\n",
        "\n",
        "    assert normalize_out.shape == test_shape,\\\n",
        "        'Incorrect Shape. {} shape found'.format(normalize_out.shape)\n",
        "\n",
        "    assert normalize_out.max() <= 1 and normalize_out.min() >= 0,\\\n",
        "        'Incorect Range. {} to {} found'.format(normalize_out.min(), normalize_out.max())\n",
        "\n",
        "    _print_success_message()\n",
        "\n",
        "\n",
        "def test_one_hot_encode(one_hot_encode):\n",
        "    test_shape = np.random.choice(range(1000))\n",
        "    test_numbers = np.random.choice(range(10), test_shape)\n",
        "    one_hot_out = one_hot_encode(test_numbers)\n",
        "\n",
        "    assert type(one_hot_out).__module__ == np.__name__,\\\n",
        "        'Not Numpy Object'\n",
        "\n",
        "    assert one_hot_out.shape == (test_shape, 10),\\\n",
        "        'Incorrect Shape. {} shape found'.format(one_hot_out.shape)\n",
        "\n",
        "    n_encode_tests = 5\n",
        "    test_pairs = list(zip(test_numbers, one_hot_out))\n",
        "    test_indices = np.random.choice(len(test_numbers), n_encode_tests)\n",
        "    labels = [test_pairs[test_i][0] for test_i in test_indices]\n",
        "    enc_labels = np.array([test_pairs[test_i][1] for test_i in test_indices])\n",
        "    new_enc_labels = one_hot_encode(labels)\n",
        "\n",
        "    assert np.array_equal(enc_labels, new_enc_labels),\\\n",
        "        'Encodings returned different results for the same numbers.\\n' \\\n",
        "        'For the first call it returned:\\n' \\\n",
        "        '{}\\n' \\\n",
        "        'For the second call it returned\\n' \\\n",
        "        '{}\\n' \\\n",
        "        'Make sure you save the map of labels to encodings outside of the function.'.format(enc_labels, new_enc_labels)\n",
        "\n",
        "    _print_success_message()\n",
        "\n",
        "\n",
        "def test_nn_image_inputs(neural_net_image_input):\n",
        "    image_shape = (32, 32, 3)\n",
        "    nn_inputs_out_x = neural_net_image_input(image_shape)\n",
        "\n",
        "    assert nn_inputs_out_x.get_shape().as_list() == [None, image_shape[0], image_shape[1], image_shape[2]],\\\n",
        "        'Incorrect Image Shape.  Found {} shape'.format(nn_inputs_out_x.get_shape().as_list())\n",
        "\n",
        "    assert nn_inputs_out_x.op.type == 'Placeholder',\\\n",
        "        'Incorrect Image Type.  Found {} type'.format(nn_inputs_out_x.op.type)\n",
        "\n",
        "    assert nn_inputs_out_x.name == 'x:0', \\\n",
        "        'Incorrect Name.  Found {}'.format(nn_inputs_out_x.name)\n",
        "\n",
        "    print('Image Input Tests Passed.')\n",
        "\n",
        "\n",
        "def test_nn_label_inputs(neural_net_label_input):\n",
        "    n_classes = 10\n",
        "    nn_inputs_out_y = neural_net_label_input(n_classes)\n",
        "\n",
        "    assert nn_inputs_out_y.get_shape().as_list() == [None, n_classes],\\\n",
        "        'Incorrect Label Shape.  Found {} shape'.format(nn_inputs_out_y.get_shape().as_list())\n",
        "\n",
        "    assert nn_inputs_out_y.op.type == 'Placeholder',\\\n",
        "        'Incorrect Label Type.  Found {} type'.format(nn_inputs_out_y.op.type)\n",
        "\n",
        "    assert nn_inputs_out_y.name == 'y:0', \\\n",
        "        'Incorrect Name.  Found {}'.format(nn_inputs_out_y.name)\n",
        "\n",
        "    print('Label Input Tests Passed.')\n",
        "\n",
        "\n",
        "def test_nn_keep_prob_inputs(neural_net_keep_prob_input):\n",
        "    nn_inputs_out_k = neural_net_keep_prob_input()\n",
        "\n",
        "    assert nn_inputs_out_k.get_shape().ndims is None,\\\n",
        "        'Too many dimensions found for keep prob.  Found {} dimensions.  It should be a scalar (0-Dimension Tensor).'.format(nn_inputs_out_k.get_shape().ndims)\n",
        "\n",
        "    assert nn_inputs_out_k.op.type == 'Placeholder',\\\n",
        "        'Incorrect keep prob Type.  Found {} type'.format(nn_inputs_out_k.op.type)\n",
        "\n",
        "    assert nn_inputs_out_k.name == 'keep_prob:0', \\\n",
        "        'Incorrect Name.  Found {}'.format(nn_inputs_out_k.name)\n",
        "\n",
        "    print('Keep Prob Tests Passed.')\n",
        "\n",
        "\n",
        "def test_con_pool(conv2d_maxpool):\n",
        "    test_x = tf.placeholder(tf.float32, [None, 32, 32, 5])\n",
        "    test_num_outputs = 10\n",
        "    test_con_k = (2, 2)\n",
        "    test_con_s = (4, 4)\n",
        "    test_pool_k = (2, 2)\n",
        "    test_pool_s = (2, 2)\n",
        "\n",
        "    conv2d_maxpool_out = conv2d_maxpool(test_x, test_num_outputs, test_con_k, test_con_s, test_pool_k, test_pool_s)\n",
        "\n",
        "    assert conv2d_maxpool_out.get_shape().as_list() == [None, 4, 4, 10],\\\n",
        "        'Incorrect Shape.  Found {} shape'.format(conv2d_maxpool_out.get_shape().as_list())\n",
        "\n",
        "    _print_success_message()\n",
        "\n",
        "\n",
        "def test_flatten(flatten):\n",
        "    test_x = tf.placeholder(tf.float32, [None, 10, 30, 6])\n",
        "    flat_out = flatten(test_x)\n",
        "\n",
        "    assert flat_out.get_shape().as_list() == [None, 10*30*6],\\\n",
        "        'Incorrect Shape.  Found {} shape'.format(flat_out.get_shape().as_list())\n",
        "\n",
        "    _print_success_message()\n",
        "\n",
        "\n",
        "def test_fully_conn(fully_conn):\n",
        "    test_x = tf.placeholder(tf.float32, [None, 128])\n",
        "    test_num_outputs = 40\n",
        "\n",
        "    fc_out = fully_conn(test_x, test_num_outputs)\n",
        "\n",
        "    assert fc_out.get_shape().as_list() == [None, 40],\\\n",
        "        'Incorrect Shape.  Found {} shape'.format(fc_out.get_shape().as_list())\n",
        "\n",
        "    _print_success_message()\n",
        "\n",
        "\n",
        "def test_output(output):\n",
        "    test_x = tf.placeholder(tf.float32, [None, 128])\n",
        "    test_num_outputs = 40\n",
        "\n",
        "    output_out = output(test_x, test_num_outputs)\n",
        "\n",
        "    assert output_out.get_shape().as_list() == [None, 40],\\\n",
        "        'Incorrect Shape.  Found {} shape'.format(output_out.get_shape().as_list())\n",
        "\n",
        "    _print_success_message()\n",
        "\n",
        "\n",
        "def test_conv_net(conv_net):\n",
        "    test_x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
        "    test_k = tf.placeholder(tf.float32)\n",
        "\n",
        "    logits_out = conv_net(test_x, test_k)\n",
        "\n",
        "    assert logits_out.get_shape().as_list() == [None, 10],\\\n",
        "        'Incorrect Model Output.  Found {}'.format(logits_out.get_shape().as_list())\n",
        "\n",
        "    print('Neural Network Built!')\n",
        "\n",
        "\n",
        "def test_train_nn(train_neural_network):\n",
        "    mock_session = tf.Session()\n",
        "    test_x = np.random.rand(128, 32, 32, 3)\n",
        "    test_y = np.random.rand(128, 10)\n",
        "    test_k = np.random.rand(1)\n",
        "    test_optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "    mock_session.run = MagicMock()\n",
        "    train_neural_network(mock_session, test_optimizer, test_k, test_x, test_y)\n",
        "\n",
        "    assert mock_session.run.called, 'Session not used'\n",
        "\n",
        "    _print_success_message()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-EDrquNcKr98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Implement Preprocess Functions\n",
        "\n",
        "# Normalize\n",
        "\n",
        "In the cell below, implement the normalize function to take in image data, x, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive. The return object should be the same shape as x."
      ]
    },
    {
      "metadata": {
        "id": "lL3IgHHCPbCt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d576476-8de4-4f87-8da5-40307b1f1843"
      },
      "cell_type": "code",
      "source": [
        "def normalize(x):\n",
        "    \"\"\"\n",
        "    Normalize a list of sample image data in the range of 0 to 1\n",
        "    : x: List of image data. The image shape is (32, 32, 3)\n",
        "    : return: Numpy array of normalize data\n",
        "    \"\"\"\n",
        "    return x / 255\n",
        "\n",
        "\n",
        "test_normalize(normalize)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vzVZyT1-LCyJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#One-hot encode\n"
      ]
    },
    {
      "metadata": {
        "id": "eGCs-hJxPqm1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30c2e93c-6309-4052-b7bb-6cd2f72ec703"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "def one_hot_encode(x):\n",
        "    \"\"\"\n",
        "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
        "    : x: List of sample Labels\n",
        "    : return: Numpy array of one-hot encoded labels\n",
        "    \"\"\"\n",
        "    # The possible values for labels are 0 to 9. 10 in total\n",
        "    return np.eye(10)[x]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
        "\"\"\"\n",
        "test_one_hot_encode(one_hot_encode)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ovlwt4ObLaM-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Randomize Data\n",
        "# Preprocess all the data and save it"
      ]
    },
    {
      "metadata": {
        "id": "He7ROOBFQVlE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Preprocess Training, Validation, and Testing Data\n",
        "preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lHJGwRRMLv47",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Check Point"
      ]
    },
    {
      "metadata": {
        "id": "YCL_ygG9QdyT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the Preprocessed Validation data\n",
        "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nKbmyJHQL3Zi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Build the network\n",
        "#Input"
      ]
    },
    {
      "metadata": {
        "id": "TGUrI9flQpmv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "445cc803-44b8-4ea2-c479-ec2d4cc51680"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def neural_net_image_input(image_shape):\n",
        "    \"\"\"\n",
        "    Return a Tensor for a batch of image input\n",
        "    : image_shape: Shape of the images\n",
        "    : return: Tensor for image input.\n",
        "    \"\"\"\n",
        "    return tf.placeholder(tf.float32, shape=[None, *image_shape], name = \"x\")\n",
        "\n",
        "\n",
        "def neural_net_label_input(n_classes):\n",
        "    \"\"\"\n",
        "    Return a Tensor for a batch of label input\n",
        "    : n_classes: Number of classes\n",
        "    : return: Tensor for label input.\n",
        "    \"\"\"\n",
        "    return tf.placeholder(tf.float32, shape=[None, n_classes], name = \"y\")\n",
        "\n",
        "\n",
        "def neural_net_keep_prob_input():\n",
        "    \"\"\"\n",
        "    Return a Tensor for keep probability\n",
        "    : return: Tensor for keep probability.\n",
        "    \"\"\"\n",
        "    return tf.placeholder(tf.float32, name=\"keep_prob\")\n",
        "\n",
        "  \n",
        "tf.reset_default_graph()\n",
        "test_nn_image_inputs(neural_net_image_input)\n",
        "test_nn_label_inputs(neural_net_label_input)\n",
        "test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image Input Tests Passed.\n",
            "Label Input Tests Passed.\n",
            "Keep Prob Tests Passed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DxHIVOQ_MC_D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Convolution and Max Pooling Layer\n",
        "Convolution layers have a lot of success with images. For this code cell, you should implement the function conv2d_maxpool to apply convolution then max pooling:\n",
        "\n",
        "--Create the weight and bias using conv_ksize, conv_num_outputs and the shape of x_tensor.\n",
        "--Apply a convolution to x_tensor using weight and conv_strides.\n",
        "--use same padding, but you're welcome to use any padding.\n",
        "--Add bias\n",
        "--Add a nonlinear activation to the convolution.\n",
        "--Apply Max Pooling using pool_ksize and pool_strides."
      ]
    },
    {
      "metadata": {
        "id": "XLKHrSLmQ39y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "36f437ff-2fac-47e4-9713-421354d45ff3"
      },
      "cell_type": "code",
      "source": [
        "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
        "    \"\"\"\n",
        "    Apply convolution then max pooling to x_tensor\n",
        "    :param x_tensor: TensorFlow Tensor\n",
        "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
        "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
        "    :param conv_strides: Stride 2-D Tuple for convolution\n",
        "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
        "    :param pool_strides: Stride 2-D Tuple for pool\n",
        "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
        "    \"\"\"\n",
        "    # x_tensor.get_shape()\n",
        "    # >> (?, 32, 32, 5)\n",
        "    input_depth = x_tensor.shape[-1].value\n",
        "\n",
        "    weights = tf.Variable(\n",
        "        tf.truncated_normal(\n",
        "            shape=[\n",
        "                conv_ksize[0], # height\n",
        "                conv_ksize[1], # width\n",
        "                input_depth, # input_depth\n",
        "                conv_num_outputs # out_depth\n",
        "            ], \n",
        "            mean=0.0,\n",
        "            stddev=0.1\n",
        "        ),\n",
        "        name='weights'\n",
        "    )\n",
        "    bias = tf.Variable(tf.zeros(conv_num_outputs), trainable=True)\n",
        "    \n",
        "    # Apply a convolution to x_tensor using weight and conv_strides\n",
        "    conv_layer = tf.nn.conv2d(\n",
        "        x_tensor, \n",
        "        weights, \n",
        "        strides=[1, *conv_strides, 1], \n",
        "        padding='SAME'\n",
        "    )\n",
        "    \n",
        "    # Add bias\n",
        "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
        "    \n",
        "    # Add a nonlinear activation to the convolution\n",
        "    conv_layer = tf.nn.relu(conv_layer)\n",
        "    \n",
        "    # Apply Max Pooling using pool_ksize and pool_strides\n",
        "    conv_layer = tf.nn.max_pool(\n",
        "        conv_layer, \n",
        "        ksize=[1, *pool_ksize, 1], \n",
        "        strides=[1, *pool_strides, 1], \n",
        "        padding='SAME'\n",
        "    )\n",
        "    \n",
        "    return conv_layer \n",
        "test_con_pool(conv2d_maxpool)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1jUFg2G6MGIS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Flatten Layer\n",
        "Implementing the flatten function to change the dimension of x_tensor from a 4-D tensor to a 2-D tensor. The output should be the shape (Batch Size, Flattened Image Size). "
      ]
    },
    {
      "metadata": {
        "id": "WGreZ89FROyF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d22b7a8-0e7b-47a1-9798-6a10b7000b92"
      },
      "cell_type": "code",
      "source": [
        "def flatten(x_tensor):\n",
        "    \"\"\"\n",
        "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
        "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
        "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
        "    \"\"\"\n",
        "    # print(x_tensor.get_shape()[1:4].num_elements())\n",
        "    return tf.reshape(x_tensor, [-1, x_tensor.get_shape()[1:4].num_elements()])\n",
        "\n",
        "\n",
        "test_flatten(flatten)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I2odnXyiMsox",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Fully-Connected Layer\n",
        "Implement the fully_conn function to apply a fully connected layer to x_tensor with the shape (Batch Size, num_outputs)."
      ]
    },
    {
      "metadata": {
        "id": "Z3nzV-3mRVty",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db4cd813-21a6-4491-d31e-8b3b7bb4aec0"
      },
      "cell_type": "code",
      "source": [
        "def fully_conn(x_tensor, num_outputs):\n",
        "    \"\"\"\n",
        "    Apply a fully connected layer to x_tensor using weight and bias\n",
        "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
        "    : num_outputs: The number of output that the new tensor should be.\n",
        "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
        "    \"\"\"\n",
        "    batch_size = x_tensor.shape[1].value\n",
        "    weights = tf.Variable(\n",
        "        tf.truncated_normal(\n",
        "            [batch_size, num_outputs],\n",
        "            mean = 0,\n",
        "            stddev=0.1)\n",
        "    )\n",
        "    bias = tf.Variable(tf.zeros(num_outputs))\n",
        "\n",
        "    fully_conn_layer = tf.matmul(x_tensor, weights)\n",
        "    fully_conn_layer = tf.nn.bias_add(fully_conn_layer, bias)\n",
        "    fully_conn_layer = tf.nn.relu(fully_conn_layer)\n",
        "\n",
        "    return fully_conn_layer\n",
        "\n",
        "test_fully_conn(fully_conn)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wgk_007CM0Q4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Output Layer\n",
        "Implement the output function to apply a fully connected layer to x_tensor with the shape (Batch Size, num_outputs)."
      ]
    },
    {
      "metadata": {
        "id": "K4A8hR1tReaZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "966d5482-14bd-4f73-e779-18adc5ee3814"
      },
      "cell_type": "code",
      "source": [
        "def output(x_tensor, num_outputs):\n",
        "    \"\"\"\n",
        "    Apply a output layer to x_tensor using weight and bias\n",
        "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
        "    : num_outputs: The number of output that the new tensor should be.\n",
        "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
        "    \"\"\"\n",
        "    batch_size = x_tensor.shape[1].value\n",
        "    weights = tf.Variable(\n",
        "        tf.truncated_normal(\n",
        "            [batch_size, num_outputs],\n",
        "            mean = 0,\n",
        "            stddev=0.1)\n",
        "    )\n",
        "    bias = tf.Variable(tf.zeros(num_outputs))\n",
        "    \n",
        "    output_layer = tf.matmul(x_tensor, weights)\n",
        "    output_layer = tf.nn.bias_add(output_layer, bias)\n",
        "    \n",
        "    return output_layer\n",
        "\n",
        "test_output(output)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d7iB2JJhM8y3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Create Convolutional Model\n",
        "Implement the function conv_net to create a convolutional neural network model. The function takes in a batch of images, x, and outputs logits. Use the layers you created above to create this model:\n",
        "\n",
        "---Apply 1, 2, or 3 Convolution and Max Pool layers\n",
        "---Apply a Flatten Layer\n",
        "---Apply 1, 2, or 3 Fully Connected Layers\n",
        "---Apply an Output Layer\n",
        "---Return the output\n",
        "---Apply TensorFlow's Dropout to one or more layers in the model using keep_prob."
      ]
    },
    {
      "metadata": {
        "id": "xbSc-SB6RlcL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e2d6772-f26b-4e90-f375-dad8bb6da80b"
      },
      "cell_type": "code",
      "source": [
        "def conv_net(x, keep_prob):\n",
        "    \"\"\"\n",
        "    Create a convolutional neural network model\n",
        "    : x: Placeholder tensor that holds image data.\n",
        "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
        "    : return: Tensor that represents logits\n",
        "    \"\"\"\n",
        "    conv_num_outputs_layer1 = 32\n",
        "    conv_num_outputs_layer2 = 64\n",
        "    conv_num_outputs_layer3 = 128\n",
        "    fully_conn_num_outputs_layer_1 = 256\n",
        "    fully_conn_num_outputs_layer_2 = 512\n",
        "    conv_ksize = (5, 5)\n",
        "    conv_strides = (1, 1)\n",
        "    pool_ksize = (2, 2)\n",
        "    pool_strides = (2, 2)\n",
        "    \n",
        "    common_params = [conv_ksize, conv_strides, pool_ksize, pool_strides]\n",
        "    \n",
        "    # Apply 1, 2, or 3 Convolution and Max Pool layers\n",
        "    conv_layer_1 = conv2d_maxpool(x, conv_num_outputs_layer1, *common_params)\n",
        "    conv_layer_1 = tf.nn.dropout(conv_layer_1, keep_prob)\n",
        "    \n",
        "    conv_layer_2 = conv2d_maxpool(conv_layer_1, conv_num_outputs_layer2, *common_params)\n",
        "    conv_layer_2 = tf.nn.dropout(conv_layer_2, keep_prob)\n",
        "    \n",
        "    conv_layer_3 = conv2d_maxpool(conv_layer_2, conv_num_outputs_layer3, *common_params)\n",
        "    conv_layer_3 = tf.nn.dropout(conv_layer_3, keep_prob)\n",
        "    \n",
        "    # Apply a Flatten Layer\n",
        "    flatten_layer_1 = flatten(conv_layer_3)\n",
        "\n",
        "    # Apply 1, 2, or 3 Fully Connected Layers\n",
        "    fully_conn_layer_1 = fully_conn(flatten_layer_1, fully_conn_num_outputs_layer_1)\n",
        "    fully_conn_layer_1 = tf.nn.dropout(fully_conn_layer_1, keep_prob)\n",
        "    fully_conn_layer_2 = fully_conn(flatten_layer_1, fully_conn_num_outputs_layer_2)\n",
        "    fully_conn_layer_2 = tf.nn.dropout(fully_conn_layer_2, keep_prob)\n",
        "    \n",
        "    \n",
        "    # Apply an Output Layer\n",
        "    num_outputs = 10 # 10 classes\n",
        "    output_layer = output(fully_conn_layer_2, num_outputs)\n",
        "    \n",
        "    return output_layer\n",
        "\n",
        "\n",
        "\n",
        "##############################\n",
        "## Build the Neural Network ##\n",
        "##############################\n",
        "\n",
        "# Remove previous weights, bias, inputs, etc..\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Inputs\n",
        "x = neural_net_image_input((32, 32, 3))\n",
        "y = neural_net_label_input(10)\n",
        "keep_prob = neural_net_keep_prob_input()\n",
        "\n",
        "# Model\n",
        "logits = conv_net(x, keep_prob)\n",
        "# Name logits Tensor, so that is can be loaded from disk after training\n",
        "logits = tf.identity(logits, name='logits')\n",
        "\n",
        "# Loss and Optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
        "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
        "\n",
        "# Accuracy\n",
        "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
        "\n",
        "test_conv_net(conv_net)\n",
        "\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neural Network Built!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aW0C7wKANenx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Train the Neural Network\n",
        "#Single Optimization\n",
        "Implement the function train_neural_network to do a single optimization. The optimization should use optimizer to optimize in session with a feed_dict of the following:\n",
        "\n",
        "---x for image input\n",
        "---y for labels\n",
        "---keep_prob for keep probability for dropout\n",
        "---This function will be called for each batch, so tf.global_variables_initializer() has already been called."
      ]
    },
    {
      "metadata": {
        "id": "oH3YOICTR5qV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64910837-deb3-4004-8735-951652b7312b"
      },
      "cell_type": "code",
      "source": [
        "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
        "    \"\"\"\n",
        "    Optimize the session on a batch of images and labels\n",
        "    : session: Current TensorFlow session\n",
        "    : optimizer: TensorFlow optimizer function\n",
        "    : keep_probability: keep probability\n",
        "    : feature_batch: Batch of Numpy image data\n",
        "    : label_batch: Batch of Numpy label data\n",
        "    \"\"\"\n",
        "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
        "\n",
        "test_train_nn(train_neural_network)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tests Passed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F_kXTV3bNt5S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Show Stats\n",
        "Implement the function print_stats to print loss and validation accuracy. Use the global variables valid_features and valid_labels to calculate validation accuracy. Use a keep probability of 1.0 to calculate the loss and validation accuracy."
      ]
    },
    {
      "metadata": {
        "id": "CSHpc-8uR_i0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
        "    \"\"\"\n",
        "    Print information about loss and validation accuracy\n",
        "    : session: Current TensorFlow session\n",
        "    : feature_batch: Batch of Numpy image data\n",
        "    : label_batch: Batch of Numpy label data\n",
        "    : cost: TensorFlow cost function\n",
        "    : accuracy: TensorFlow accuracy function\n",
        "    \"\"\"\n",
        "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
        "    accuracy = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
        "    print(\"Loss: {} Accuracy: {}\".format(loss, accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u1CZJh7ZNxKl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "qB_GKtfpSE7s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: Tune Parameters\n",
        "epochs = 35\n",
        "batch_size = 128\n",
        "keep_probability = 0.75"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-uARXEhLN2EA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Train on a Single CIFAR-10 Batch"
      ]
    },
    {
      "metadata": {
        "id": "smahesn_SIp2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "2895efa0-cffd-4f74-d172-7af09e9623c3"
      },
      "cell_type": "code",
      "source": [
        "print('Checking the Training on a Single Batch...')\n",
        "with tf.Session() as sess:\n",
        "    # Initializing the variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Training cycle\n",
        "    for epoch in range(epochs):\n",
        "        batch_i = 1\n",
        "        for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
        "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
        "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
        "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
        "        \n",
        "        \n",
        "        \n",
        "       "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the Training on a Single Batch...\n",
            "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.20892333984375 Accuracy: 0.2476000040769577\n",
            "Epoch  2, CIFAR-10 Batch 1:  Loss: 1.9704746007919312 Accuracy: 0.3668000102043152\n",
            "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.8481582403182983 Accuracy: 0.4052000045776367\n",
            "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.7085129022598267 Accuracy: 0.42340001463890076\n",
            "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.5978643894195557 Accuracy: 0.453000009059906\n",
            "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.4473758935928345 Accuracy: 0.46619999408721924\n",
            "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.3690979480743408 Accuracy: 0.47920000553131104\n",
            "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.2559802532196045 Accuracy: 0.5044000148773193\n",
            "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.2660491466522217 Accuracy: 0.5022000074386597\n",
            "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.0980839729309082 Accuracy: 0.5180000066757202\n",
            "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.0543686151504517 Accuracy: 0.527999997138977\n",
            "Epoch 12, CIFAR-10 Batch 1:  Loss: 0.9794113039970398 Accuracy: 0.5414000153541565\n",
            "Epoch 13, CIFAR-10 Batch 1:  Loss: 0.9334124326705933 Accuracy: 0.550000011920929\n",
            "Epoch 14, CIFAR-10 Batch 1:  Loss: 0.8392101526260376 Accuracy: 0.5547999739646912\n",
            "Epoch 15, CIFAR-10 Batch 1:  Loss: 0.8232203722000122 Accuracy: 0.5595999956130981\n",
            "Epoch 16, CIFAR-10 Batch 1:  Loss: 0.7566943168640137 Accuracy: 0.5680000185966492\n",
            "Epoch 17, CIFAR-10 Batch 1:  Loss: 0.7867633104324341 Accuracy: 0.5537999868392944\n",
            "Epoch 18, CIFAR-10 Batch 1:  Loss: 0.6072322130203247 Accuracy: 0.5821999907493591\n",
            "Epoch 19, CIFAR-10 Batch 1:  Loss: 0.6334916353225708 Accuracy: 0.5748000144958496\n",
            "Epoch 20, CIFAR-10 Batch 1:  Loss: 0.5557417273521423 Accuracy: 0.5917999744415283\n",
            "Epoch 21, CIFAR-10 Batch 1:  Loss: 0.593065619468689 Accuracy: 0.58160001039505\n",
            "Epoch 22, CIFAR-10 Batch 1:  Loss: 0.4850475788116455 Accuracy: 0.5916000008583069\n",
            "Epoch 23, CIFAR-10 Batch 1:  Loss: 0.44289737939834595 Accuracy: 0.5932000279426575\n",
            "Epoch 24, CIFAR-10 Batch 1:  Loss: 0.3804070055484772 Accuracy: 0.6000000238418579\n",
            "Epoch 25, CIFAR-10 Batch 1:  Loss: 0.3447142541408539 Accuracy: 0.6141999959945679\n",
            "Epoch 26, CIFAR-10 Batch 1:  Loss: 0.30626723170280457 Accuracy: 0.621999979019165\n",
            "Epoch 27, CIFAR-10 Batch 1:  Loss: 0.2738483250141144 Accuracy: 0.6083999872207642\n",
            "Epoch 28, CIFAR-10 Batch 1:  Loss: 0.20571616291999817 Accuracy: 0.6222000122070312\n",
            "Epoch 29, CIFAR-10 Batch 1:  Loss: 0.21926212310791016 Accuracy: 0.620199978351593\n",
            "Epoch 30, CIFAR-10 Batch 1:  Loss: 0.1427173912525177 Accuracy: 0.6327999830245972\n",
            "Epoch 31, CIFAR-10 Batch 1:  Loss: 0.16988587379455566 Accuracy: 0.6442000269889832\n",
            "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.15733620524406433 Accuracy: 0.6425999999046326\n",
            "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.1147436872124672 Accuracy: 0.6394000053405762\n",
            "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.10811755806207657 Accuracy: 0.6439999938011169\n",
            "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.09570830315351486 Accuracy: 0.6355999708175659\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oLg0znVWN9Je",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Fully Train the Model"
      ]
    },
    {
      "metadata": {
        "id": "xlI00gspcQoP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3009
        },
        "outputId": "a791a5ff-b49e-4340-9ddf-11dcf65492c6"
      },
      "cell_type": "code",
      "source": [
        "save_model_path = './image_classification'\n",
        "\n",
        "print('Training...')\n",
        "with tf.Session() as sess:\n",
        "    # Initializing the variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Training cycle\n",
        "    for epoch in range(epochs):\n",
        "        # Loop over all batches\n",
        "        n_batches = 5\n",
        "        for batch_i in range(1, n_batches + 1):\n",
        "            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
        "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
        "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
        "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
        "            \n",
        "    # Save Model\n",
        "    saver = tf.train.Saver()\n",
        "    save_path = saver.save(sess, save_model_path)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.2124199867248535 Accuracy: 0.21040000021457672\n",
            "Epoch  1, CIFAR-10 Batch 2:  Loss: 2.061746597290039 Accuracy: 0.3142000138759613\n",
            "Epoch  1, CIFAR-10 Batch 3:  Loss: 1.6801704168319702 Accuracy: 0.35740000009536743\n",
            "Epoch  1, CIFAR-10 Batch 4:  Loss: 1.730249047279358 Accuracy: 0.4023999869823456\n",
            "Epoch  1, CIFAR-10 Batch 5:  Loss: 1.7177143096923828 Accuracy: 0.4036000072956085\n",
            "Epoch  2, CIFAR-10 Batch 1:  Loss: 1.7214339971542358 Accuracy: 0.4472000002861023\n",
            "Epoch  2, CIFAR-10 Batch 2:  Loss: 1.5520470142364502 Accuracy: 0.4212000072002411\n",
            "Epoch  2, CIFAR-10 Batch 3:  Loss: 1.3437557220458984 Accuracy: 0.46959999203681946\n",
            "Epoch  2, CIFAR-10 Batch 4:  Loss: 1.5267083644866943 Accuracy: 0.4848000109195709\n",
            "Epoch  2, CIFAR-10 Batch 5:  Loss: 1.4107539653778076 Accuracy: 0.49160000681877136\n",
            "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.5365463495254517 Accuracy: 0.5013999938964844\n",
            "Epoch  3, CIFAR-10 Batch 2:  Loss: 1.3846639394760132 Accuracy: 0.5052000284194946\n",
            "Epoch  3, CIFAR-10 Batch 3:  Loss: 1.2084951400756836 Accuracy: 0.5098000168800354\n",
            "Epoch  3, CIFAR-10 Batch 4:  Loss: 1.3442469835281372 Accuracy: 0.525600016117096\n",
            "Epoch  3, CIFAR-10 Batch 5:  Loss: 1.2264702320098877 Accuracy: 0.5289999842643738\n",
            "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.3549234867095947 Accuracy: 0.5482000112533569\n",
            "Epoch  4, CIFAR-10 Batch 2:  Loss: 1.2558605670928955 Accuracy: 0.5378000140190125\n",
            "Epoch  4, CIFAR-10 Batch 3:  Loss: 1.0851716995239258 Accuracy: 0.5529999732971191\n",
            "Epoch  4, CIFAR-10 Batch 4:  Loss: 1.2155508995056152 Accuracy: 0.5726000070571899\n",
            "Epoch  4, CIFAR-10 Batch 5:  Loss: 1.1258246898651123 Accuracy: 0.569599986076355\n",
            "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.1647049188613892 Accuracy: 0.5857999920845032\n",
            "Epoch  5, CIFAR-10 Batch 2:  Loss: 1.1590251922607422 Accuracy: 0.5641999840736389\n",
            "Epoch  5, CIFAR-10 Batch 3:  Loss: 0.946816086769104 Accuracy: 0.5983999967575073\n",
            "Epoch  5, CIFAR-10 Batch 4:  Loss: 1.09514582157135 Accuracy: 0.592199981212616\n",
            "Epoch  5, CIFAR-10 Batch 5:  Loss: 0.9893781542778015 Accuracy: 0.5861999988555908\n",
            "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.0441620349884033 Accuracy: 0.6101999878883362\n",
            "Epoch  6, CIFAR-10 Batch 2:  Loss: 1.05060613155365 Accuracy: 0.6197999715805054\n",
            "Epoch  6, CIFAR-10 Batch 3:  Loss: 0.7911383509635925 Accuracy: 0.618399977684021\n",
            "Epoch  6, CIFAR-10 Batch 4:  Loss: 0.9102304577827454 Accuracy: 0.6176000237464905\n",
            "Epoch  6, CIFAR-10 Batch 5:  Loss: 0.8468788266181946 Accuracy: 0.6164000034332275\n",
            "Epoch  7, CIFAR-10 Batch 1:  Loss: 0.8722322583198547 Accuracy: 0.6317999958992004\n",
            "Epoch  7, CIFAR-10 Batch 2:  Loss: 0.8984464406967163 Accuracy: 0.6366000175476074\n",
            "Epoch  7, CIFAR-10 Batch 3:  Loss: 0.6835623383522034 Accuracy: 0.6517999768257141\n",
            "Epoch  7, CIFAR-10 Batch 4:  Loss: 0.8326863050460815 Accuracy: 0.6340000033378601\n",
            "Epoch  7, CIFAR-10 Batch 5:  Loss: 0.7025514841079712 Accuracy: 0.6417999863624573\n",
            "Epoch  8, CIFAR-10 Batch 1:  Loss: 0.7746216058731079 Accuracy: 0.6675999760627747\n",
            "Epoch  8, CIFAR-10 Batch 2:  Loss: 0.7392587661743164 Accuracy: 0.6547999978065491\n",
            "Epoch  8, CIFAR-10 Batch 3:  Loss: 0.570208728313446 Accuracy: 0.6686000227928162\n",
            "Epoch  8, CIFAR-10 Batch 4:  Loss: 0.7725287675857544 Accuracy: 0.6424000263214111\n",
            "Epoch  8, CIFAR-10 Batch 5:  Loss: 0.6108640432357788 Accuracy: 0.6639999747276306\n",
            "Epoch  9, CIFAR-10 Batch 1:  Loss: 0.7130831480026245 Accuracy: 0.6880000233650208\n",
            "Epoch  9, CIFAR-10 Batch 2:  Loss: 0.7166434526443481 Accuracy: 0.6714000105857849\n",
            "Epoch  9, CIFAR-10 Batch 3:  Loss: 0.491093248128891 Accuracy: 0.6765999794006348\n",
            "Epoch  9, CIFAR-10 Batch 4:  Loss: 0.5791863799095154 Accuracy: 0.6836000084877014\n",
            "Epoch  9, CIFAR-10 Batch 5:  Loss: 0.5634474754333496 Accuracy: 0.6773999929428101\n",
            "Epoch 10, CIFAR-10 Batch 1:  Loss: 0.604119598865509 Accuracy: 0.692799985408783\n",
            "Epoch 10, CIFAR-10 Batch 2:  Loss: 0.619503378868103 Accuracy: 0.6761999726295471\n",
            "Epoch 10, CIFAR-10 Batch 3:  Loss: 0.4895329475402832 Accuracy: 0.6959999799728394\n",
            "Epoch 10, CIFAR-10 Batch 4:  Loss: 0.5457952618598938 Accuracy: 0.6985999941825867\n",
            "Epoch 10, CIFAR-10 Batch 5:  Loss: 0.5272444486618042 Accuracy: 0.6837999820709229\n",
            "Epoch 11, CIFAR-10 Batch 1:  Loss: 0.49740880727767944 Accuracy: 0.7080000042915344\n",
            "Epoch 11, CIFAR-10 Batch 2:  Loss: 0.4858178496360779 Accuracy: 0.7192000150680542\n",
            "Epoch 11, CIFAR-10 Batch 3:  Loss: 0.47219887375831604 Accuracy: 0.7008000016212463\n",
            "Epoch 11, CIFAR-10 Batch 4:  Loss: 0.5598179697990417 Accuracy: 0.7074000239372253\n",
            "Epoch 11, CIFAR-10 Batch 5:  Loss: 0.43731850385665894 Accuracy: 0.6998000144958496\n",
            "Epoch 12, CIFAR-10 Batch 1:  Loss: 0.5256556868553162 Accuracy: 0.7170000076293945\n",
            "Epoch 12, CIFAR-10 Batch 2:  Loss: 0.4475504755973816 Accuracy: 0.6899999976158142\n",
            "Epoch 12, CIFAR-10 Batch 3:  Loss: 0.3913783133029938 Accuracy: 0.7203999757766724\n",
            "Epoch 12, CIFAR-10 Batch 4:  Loss: 0.46663421392440796 Accuracy: 0.7093999981880188\n",
            "Epoch 12, CIFAR-10 Batch 5:  Loss: 0.36750203371047974 Accuracy: 0.7128000259399414\n",
            "Epoch 13, CIFAR-10 Batch 1:  Loss: 0.4192849099636078 Accuracy: 0.7020000219345093\n",
            "Epoch 13, CIFAR-10 Batch 2:  Loss: 0.3840900659561157 Accuracy: 0.7113999724388123\n",
            "Epoch 13, CIFAR-10 Batch 3:  Loss: 0.3411487936973572 Accuracy: 0.7301999926567078\n",
            "Epoch 13, CIFAR-10 Batch 4:  Loss: 0.4016762375831604 Accuracy: 0.7229999899864197\n",
            "Epoch 13, CIFAR-10 Batch 5:  Loss: 0.4060140550136566 Accuracy: 0.7106000185012817\n",
            "Epoch 14, CIFAR-10 Batch 1:  Loss: 0.3772393763065338 Accuracy: 0.7282000184059143\n",
            "Epoch 14, CIFAR-10 Batch 2:  Loss: 0.30342021584510803 Accuracy: 0.7116000056266785\n",
            "Epoch 14, CIFAR-10 Batch 3:  Loss: 0.2588391900062561 Accuracy: 0.7396000027656555\n",
            "Epoch 14, CIFAR-10 Batch 4:  Loss: 0.3634253442287445 Accuracy: 0.7342000007629395\n",
            "Epoch 14, CIFAR-10 Batch 5:  Loss: 0.33448949456214905 Accuracy: 0.7337999939918518\n",
            "Epoch 15, CIFAR-10 Batch 1:  Loss: 0.3718131482601166 Accuracy: 0.725600004196167\n",
            "Epoch 15, CIFAR-10 Batch 2:  Loss: 0.23903191089630127 Accuracy: 0.7275999784469604\n",
            "Epoch 15, CIFAR-10 Batch 3:  Loss: 0.23361697793006897 Accuracy: 0.7432000041007996\n",
            "Epoch 15, CIFAR-10 Batch 4:  Loss: 0.2860364317893982 Accuracy: 0.715399980545044\n",
            "Epoch 15, CIFAR-10 Batch 5:  Loss: 0.3175317347049713 Accuracy: 0.7214000225067139\n",
            "Epoch 16, CIFAR-10 Batch 1:  Loss: 0.32229045033454895 Accuracy: 0.7382000088691711\n",
            "Epoch 16, CIFAR-10 Batch 2:  Loss: 0.20749683678150177 Accuracy: 0.7347999811172485\n",
            "Epoch 16, CIFAR-10 Batch 3:  Loss: 0.2330094277858734 Accuracy: 0.7278000116348267\n",
            "Epoch 16, CIFAR-10 Batch 4:  Loss: 0.24573931097984314 Accuracy: 0.7360000014305115\n",
            "Epoch 16, CIFAR-10 Batch 5:  Loss: 0.21586978435516357 Accuracy: 0.7494000196456909\n",
            "Epoch 17, CIFAR-10 Batch 1:  Loss: 0.2315814048051834 Accuracy: 0.73580002784729\n",
            "Epoch 17, CIFAR-10 Batch 2:  Loss: 0.22011741995811462 Accuracy: 0.7400000095367432\n",
            "Epoch 17, CIFAR-10 Batch 3:  Loss: 0.1745496541261673 Accuracy: 0.730400025844574\n",
            "Epoch 17, CIFAR-10 Batch 4:  Loss: 0.1864507496356964 Accuracy: 0.7516000270843506\n",
            "Epoch 17, CIFAR-10 Batch 5:  Loss: 0.21506324410438538 Accuracy: 0.7462000250816345\n",
            "Epoch 18, CIFAR-10 Batch 1:  Loss: 0.20893481373786926 Accuracy: 0.7477999925613403\n",
            "Epoch 18, CIFAR-10 Batch 2:  Loss: 0.1601913571357727 Accuracy: 0.746999979019165\n",
            "Epoch 18, CIFAR-10 Batch 3:  Loss: 0.17830513417720795 Accuracy: 0.7487999796867371\n",
            "Epoch 18, CIFAR-10 Batch 4:  Loss: 0.1796654462814331 Accuracy: 0.7473999857902527\n",
            "Epoch 18, CIFAR-10 Batch 5:  Loss: 0.2082788497209549 Accuracy: 0.7472000122070312\n",
            "Epoch 19, CIFAR-10 Batch 1:  Loss: 0.21132583916187286 Accuracy: 0.7603999972343445\n",
            "Epoch 19, CIFAR-10 Batch 2:  Loss: 0.13772694766521454 Accuracy: 0.7540000081062317\n",
            "Epoch 19, CIFAR-10 Batch 3:  Loss: 0.14331074059009552 Accuracy: 0.7473999857902527\n",
            "Epoch 19, CIFAR-10 Batch 4:  Loss: 0.15291230380535126 Accuracy: 0.7351999878883362\n",
            "Epoch 19, CIFAR-10 Batch 5:  Loss: 0.13185927271842957 Accuracy: 0.7573999762535095\n",
            "Epoch 20, CIFAR-10 Batch 1:  Loss: 0.20302680134773254 Accuracy: 0.7630000114440918\n",
            "Epoch 20, CIFAR-10 Batch 2:  Loss: 0.17106619477272034 Accuracy: 0.7473999857902527\n",
            "Epoch 20, CIFAR-10 Batch 3:  Loss: 0.15151648223400116 Accuracy: 0.7440000176429749\n",
            "Epoch 20, CIFAR-10 Batch 4:  Loss: 0.15011604130268097 Accuracy: 0.7513999938964844\n",
            "Epoch 20, CIFAR-10 Batch 5:  Loss: 0.16774654388427734 Accuracy: 0.7552000284194946\n",
            "Epoch 21, CIFAR-10 Batch 1:  Loss: 0.15066757798194885 Accuracy: 0.7508000135421753\n",
            "Epoch 21, CIFAR-10 Batch 2:  Loss: 0.09836168587207794 Accuracy: 0.7573999762535095\n",
            "Epoch 21, CIFAR-10 Batch 3:  Loss: 0.11359067261219025 Accuracy: 0.7419999837875366\n",
            "Epoch 21, CIFAR-10 Batch 4:  Loss: 0.11290726810693741 Accuracy: 0.7598000168800354\n",
            "Epoch 21, CIFAR-10 Batch 5:  Loss: 0.11194854974746704 Accuracy: 0.7639999985694885\n",
            "Epoch 22, CIFAR-10 Batch 1:  Loss: 0.13783740997314453 Accuracy: 0.7572000026702881\n",
            "Epoch 22, CIFAR-10 Batch 2:  Loss: 0.09378372132778168 Accuracy: 0.7598000168800354\n",
            "Epoch 22, CIFAR-10 Batch 3:  Loss: 0.11320779472589493 Accuracy: 0.7573999762535095\n",
            "Epoch 22, CIFAR-10 Batch 4:  Loss: 0.08670361340045929 Accuracy: 0.7531999945640564\n",
            "Epoch 22, CIFAR-10 Batch 5:  Loss: 0.0869503915309906 Accuracy: 0.7688000202178955\n",
            "Epoch 23, CIFAR-10 Batch 1:  Loss: 0.09242641925811768 Accuracy: 0.7675999999046326\n",
            "Epoch 23, CIFAR-10 Batch 2:  Loss: 0.09601390361785889 Accuracy: 0.754800021648407\n",
            "Epoch 23, CIFAR-10 Batch 3:  Loss: 0.07469023764133453 Accuracy: 0.7674000263214111\n",
            "Epoch 23, CIFAR-10 Batch 4:  Loss: 0.10427006334066391 Accuracy: 0.7555999755859375\n",
            "Epoch 23, CIFAR-10 Batch 5:  Loss: 0.10142912715673447 Accuracy: 0.7576000094413757\n",
            "Epoch 24, CIFAR-10 Batch 1:  Loss: 0.0769667848944664 Accuracy: 0.769599974155426\n",
            "Epoch 24, CIFAR-10 Batch 2:  Loss: 0.07273710519075394 Accuracy: 0.7684000134468079\n",
            "Epoch 24, CIFAR-10 Batch 3:  Loss: 0.08546242862939835 Accuracy: 0.7666000127792358\n",
            "Epoch 24, CIFAR-10 Batch 4:  Loss: 0.08614259958267212 Accuracy: 0.7630000114440918\n",
            "Epoch 24, CIFAR-10 Batch 5:  Loss: 0.08279046416282654 Accuracy: 0.7670000195503235\n",
            "Epoch 25, CIFAR-10 Batch 1:  Loss: 0.08793281018733978 Accuracy: 0.7698000073432922\n",
            "Epoch 25, CIFAR-10 Batch 2:  Loss: 0.04895571991801262 Accuracy: 0.7626000046730042\n",
            "Epoch 25, CIFAR-10 Batch 3:  Loss: 0.08369738608598709 Accuracy: 0.7540000081062317\n",
            "Epoch 25, CIFAR-10 Batch 4:  Loss: 0.11213812977075577 Accuracy: 0.76419997215271\n",
            "Epoch 25, CIFAR-10 Batch 5:  Loss: 0.06943809241056442 Accuracy: 0.7617999911308289\n",
            "Epoch 26, CIFAR-10 Batch 1:  Loss: 0.055236201733350754 Accuracy: 0.7739999890327454\n",
            "Epoch 26, CIFAR-10 Batch 2:  Loss: 0.059162892401218414 Accuracy: 0.7603999972343445\n",
            "Epoch 26, CIFAR-10 Batch 3:  Loss: 0.05526721477508545 Accuracy: 0.7630000114440918\n",
            "Epoch 26, CIFAR-10 Batch 4:  Loss: 0.08594802767038345 Accuracy: 0.76419997215271\n",
            "Epoch 26, CIFAR-10 Batch 5:  Loss: 0.04394357651472092 Accuracy: 0.7753999829292297\n",
            "Epoch 27, CIFAR-10 Batch 1:  Loss: 0.0843348577618599 Accuracy: 0.7778000235557556\n",
            "Epoch 27, CIFAR-10 Batch 2:  Loss: 0.08211050182580948 Accuracy: 0.7545999884605408\n",
            "Epoch 27, CIFAR-10 Batch 3:  Loss: 0.03627810627222061 Accuracy: 0.7605999708175659\n",
            "Epoch 27, CIFAR-10 Batch 4:  Loss: 0.0855141133069992 Accuracy: 0.7577999830245972\n",
            "Epoch 27, CIFAR-10 Batch 5:  Loss: 0.0415203832089901 Accuracy: 0.7702000141143799\n",
            "Epoch 28, CIFAR-10 Batch 1:  Loss: 0.05167924240231514 Accuracy: 0.7814000248908997\n",
            "Epoch 28, CIFAR-10 Batch 2:  Loss: 0.05364609882235527 Accuracy: 0.7554000020027161\n",
            "Epoch 28, CIFAR-10 Batch 3:  Loss: 0.03279127925634384 Accuracy: 0.7626000046730042\n",
            "Epoch 28, CIFAR-10 Batch 4:  Loss: 0.06973792612552643 Accuracy: 0.7559999823570251\n",
            "Epoch 28, CIFAR-10 Batch 5:  Loss: 0.04196916148066521 Accuracy: 0.7720000147819519\n",
            "Epoch 29, CIFAR-10 Batch 1:  Loss: 0.06206867843866348 Accuracy: 0.7730000019073486\n",
            "Epoch 29, CIFAR-10 Batch 2:  Loss: 0.0343996100127697 Accuracy: 0.7594000101089478\n",
            "Epoch 29, CIFAR-10 Batch 3:  Loss: 0.04036533460021019 Accuracy: 0.7742000222206116\n",
            "Epoch 29, CIFAR-10 Batch 4:  Loss: 0.03638497367501259 Accuracy: 0.7627999782562256\n",
            "Epoch 29, CIFAR-10 Batch 5:  Loss: 0.03290698677301407 Accuracy: 0.7612000107765198\n",
            "Epoch 30, CIFAR-10 Batch 1:  Loss: 0.04264260083436966 Accuracy: 0.7702000141143799\n",
            "Epoch 30, CIFAR-10 Batch 2:  Loss: 0.032449208199977875 Accuracy: 0.7591999769210815\n",
            "Epoch 30, CIFAR-10 Batch 3:  Loss: 0.026212677359580994 Accuracy: 0.7692000269889832\n",
            "Epoch 30, CIFAR-10 Batch 4:  Loss: 0.058672986924648285 Accuracy: 0.7613999843597412\n",
            "Epoch 30, CIFAR-10 Batch 5:  Loss: 0.041034944355487823 Accuracy: 0.777400016784668\n",
            "Epoch 31, CIFAR-10 Batch 1:  Loss: 0.04701904207468033 Accuracy: 0.7703999876976013\n",
            "Epoch 31, CIFAR-10 Batch 2:  Loss: 0.02863885834813118 Accuracy: 0.7717999815940857\n",
            "Epoch 31, CIFAR-10 Batch 3:  Loss: 0.030752157792448997 Accuracy: 0.7666000127792358\n",
            "Epoch 31, CIFAR-10 Batch 4:  Loss: 0.04294062778353691 Accuracy: 0.7833999991416931\n",
            "Epoch 31, CIFAR-10 Batch 5:  Loss: 0.04364626482129097 Accuracy: 0.7692000269889832\n",
            "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.04265517741441727 Accuracy: 0.7766000032424927\n",
            "Epoch 32, CIFAR-10 Batch 2:  Loss: 0.023177091032266617 Accuracy: 0.7738000154495239\n",
            "Epoch 32, CIFAR-10 Batch 3:  Loss: 0.05998789146542549 Accuracy: 0.7680000066757202\n",
            "Epoch 32, CIFAR-10 Batch 4:  Loss: 0.03230956569314003 Accuracy: 0.7748000025749207\n",
            "Epoch 32, CIFAR-10 Batch 5:  Loss: 0.04881054908037186 Accuracy: 0.7620000243186951\n",
            "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.029031643643975258 Accuracy: 0.7815999984741211\n",
            "Epoch 33, CIFAR-10 Batch 2:  Loss: 0.021313348785042763 Accuracy: 0.7662000060081482\n",
            "Epoch 33, CIFAR-10 Batch 3:  Loss: 0.03626087307929993 Accuracy: 0.7644000053405762\n",
            "Epoch 33, CIFAR-10 Batch 4:  Loss: 0.01995673216879368 Accuracy: 0.7694000005722046\n",
            "Epoch 33, CIFAR-10 Batch 5:  Loss: 0.048131030052900314 Accuracy: 0.7698000073432922\n",
            "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.027219057083129883 Accuracy: 0.7752000093460083\n",
            "Epoch 34, CIFAR-10 Batch 2:  Loss: 0.03893410414457321 Accuracy: 0.767799973487854\n",
            "Epoch 34, CIFAR-10 Batch 3:  Loss: 0.02257743664085865 Accuracy: 0.7580000162124634\n",
            "Epoch 34, CIFAR-10 Batch 4:  Loss: 0.023594466969370842 Accuracy: 0.775600016117096\n",
            "Epoch 34, CIFAR-10 Batch 5:  Loss: 0.01790620945394039 Accuracy: 0.7716000080108643\n",
            "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.021912455558776855 Accuracy: 0.7888000011444092\n",
            "Epoch 35, CIFAR-10 Batch 2:  Loss: 0.02390686236321926 Accuracy: 0.776199996471405\n",
            "Epoch 35, CIFAR-10 Batch 3:  Loss: 0.029186060652136803 Accuracy: 0.7598000168800354\n",
            "Epoch 35, CIFAR-10 Batch 4:  Loss: 0.03244761750102043 Accuracy: 0.7770000100135803\n",
            "Epoch 35, CIFAR-10 Batch 5:  Loss: 0.03147168084979057 Accuracy: 0.7789999842643738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T9isDK8oOGmI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Checkpoint\n",
        "The model has been saved to disk.\n",
        "\n",
        "#Test Model"
      ]
    },
    {
      "metadata": {
        "id": "rblCVk_HJHTh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "e16b49bb-2fa0-4720-a5a1-4dec02380ad3"
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Set batch size if not already set\n",
        "try:\n",
        "    if batch_size:\n",
        "        pass\n",
        "except NameError:\n",
        "    batch_size = 64\n",
        "\n",
        "save_model_path = './image_classification'\n",
        "n_samples = 4\n",
        "top_n_predictions = 3\n",
        "\n",
        "def test_model():\n",
        "    \"\"\"\n",
        "    Test the saved model against the test dataset\n",
        "    \"\"\"\n",
        "\n",
        "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
        "    loaded_graph = tf.Graph()\n",
        "\n",
        "    with tf.Session(graph=loaded_graph) as sess:\n",
        "        # Load model\n",
        "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
        "        loader.restore(sess, save_model_path)\n",
        "\n",
        "        # Get Tensors from loaded model\n",
        "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
        "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
        "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
        "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
        "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
        "        \n",
        "        # Get accuracy in batches for memory limitations\n",
        "        test_batch_acc_total = 0\n",
        "        test_batch_count = 0\n",
        "        \n",
        "        for train_feature_batch, train_label_batch in batch_features_labels(test_features, test_labels, batch_size):\n",
        "            test_batch_acc_total += sess.run(\n",
        "                loaded_acc,\n",
        "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
        "            test_batch_count += 1\n",
        "\n",
        "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
        "\n",
        "        # Print Random Samples\n",
        "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
        "        random_test_predictions = sess.run(\n",
        "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
        "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
        "        display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
        "\n",
        "test_model()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./image_classification\n",
            "Testing Accuracy: 0.767306170886076\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdgAAAG1CAYAAABas+UdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzsnXd4VVX297+33/Se0AcISehIzSgM\nSIggVkQZioSIDIgClkGGGvkFQRTQkeo7g40mOiiKgBQVFEVAUOkSqpAEEkL67e28f6x9LlwSIGDu\nTVuf5+G5nLb3PuckWfe719prKSRJksAwDMMwTKWirOoBMAzDMExthA0swzAMw3gBNrAMwzAM4wXY\nwDIMwzCMF2ADyzAMwzBegA0swzAMw3gBNrB1mAsXLmDw4MFo27YtHn744aoeTq0nKSkJSUlJ7u31\n69cjISEB69evr5T2K7s9hmH+HOqqHgBzY5xOJ9avX4/NmzfjwoULyMvLg0KhQL169dCpUyeMHDkS\nCQkJd9z+m2++iYMHD+If//gHOnbsCAAwGo14//33kZqaiuDg4Mq6FZ+yePFiLFmypMx+tVqNiIgI\ndO3aFWPGjPlTz64ySExMxMKFC9GuXbvbvjYnJwfr1q3DhAkTKqU9hmEqHzaw1RSXy4Xx48djx44d\nuPvuu5GamorIyEiUlJTg8OHD2LRpEzZv3ox3330XiYmJd9RHRkYGwsLCMGnSJPe+I0eOYMmSJXjs\nscdqrIGVSU1NRadOndzbBoMBGRkZ+PTTT7F9+/Y/9ewqg4YNG6Jhw4Z3dO3u3buxZMkSDwP7Z9pj\nGKbyYQNbTfnhhx+wY8cO9OvXD4sWLfI4NnToUDz88MMYOXIk3nzzTfzvf/+7oz5sNhv8/f099h05\ncuSOx1zd6NChA+6///4y+x966CEMHToUc+bMwZdfflkFI/vz1Kb3xDC1FfbBVlMyMjIAAD179iz3\n+D333IOlS5di5syZHvutViuWLFmCBx54AO3bt0fHjh0xaNAgrFu3zn2O7KvLzs5GdnY2EhIS3P7B\nBQsWAAD69OnjnkLdt28fEhISsGzZMnz//fd45JFH0L59eyQlJWHNmjUA6AvBwIED0aFDB/Tp0wdv\nv/02rs/CefbsWUyZMgU9evRAmzZt0KNHD4wZMwaHDx92n2OxWNCvXz907doV+fn5Htd/9dVXSEhI\nQFpa2p08UjcdOnRAbGwsMjIyUFxcDICmlRMSErBnzx5MnDgRnTp1ct8bAGRlZWHq1Kno0aMH2rZt\ni+7du2PixIk4c+ZMmfb37NmDQYMGoX379khMTMRLL72Ey5cvlznvRj7To0eP4tlnn0ViYiLatWuH\ngQMHYuvWre7jCQkJWLt2rfv/sl/3Ru0dOnQIY8eORWJiItq2bYuePXti6tSpyMrK8jgvJSUFrVu3\nhs1mw7x589CrVy+0bdsWSUlJ+PDDDz3Otdvt+PDDDzFgwAB06dIFHTt2xAMPPIBFixbBZrPd6hUw\nTJ2AFWw1JTo6GgCwfft2PPLII9BqtWXOSU5O9th2uVwYO3YsfvrpJzz44IN46qmnYLPZsHXrVsyY\nMQNZWVl46aWX3L669PR0AMDMmTPh5+cHAHj33Xfx888/Y+bMmQgPD/do//jx41i/fj2GDx8OjUaD\n5cuXY9asWbBYLFi9ejVSUlIQGBiIlStX4p133kFCQgL69+8PAMjNzcWwYcPgcrnw9NNPo2HDhsjN\nzcWqVaswbNgwrF27Fu3atYNer8fcuXPx5JNPYu7cuW6DbzAYMHfuXDRs2BCTJ0/+089Xr9cDABwO\nh8f+FStWQKFQID093f0FIzMzE4MGDYJarcaQIUPQqFEjXLhwAWvWrMF3332Hjz/+GHFxcQBIWY4e\nPRqBgYF45plnEBMTg0OHDmHUqFGw2Wzlvsdr+fXXXzFixAg0a9YML7zwAjQaDTZs2IAXXngBaWlp\nGD58OBYuXIjFixfj9OnTWLhwofvdlceePXswevRoREREIDU1FQ0aNMDp06exZs0afP/99/j8888R\nExPjcc3UqVORn5+PZ5991m1I586di0aNGrl/5mbPno2PP/4YDz74IFJSUqBSqbB//34sW7YMJ0+e\nLNcHzjB1DomplhgMBql3795SfHy81LdvX2nZsmXSb7/9Jtlsthtes3nzZik+Pl5KS0vz2G+326VH\nHnlEatWqlZSTk+Pe37t3b6l3794e506ePFmKj4+XMjMz3fv27t0rxcfHS61atZLOnz/v3v/VV19J\n8fHxUsuWLaXTp0+79x86dEiKj4+XJk2a5N73ww8/SCNGjJA2btzo0d+uXbuk+Ph4adq0aR77X3/9\ndSk+Pl766aefJEmSpDlz5kgJCQnu7ZuxaNEiKT4+Xtq0aVO5x3Nzc6UOHTp43Lt8zQMPPFDmGY8b\nN07q2LGjx71LkiT9/vvvUqtWraSxY8d6nHvtuGWWLFkixcfHe/T52WefSfHx8dJnn33m3jdgwACp\na9euUmFhoXuf1WqVkpKSpE6dOkkWi0WSJEkaPny4FB8f79FHee3df//9Urt27aQLFy54nCu/u1de\necW9T25zzJgxksvlcu/fv39/mffZqVMn6cEHH5Su5z//+Y80btw4yWg0ljnGMHUNniKupgQEBGDt\n2rXo27cvzp8/j7fffhuDBw9Gly5dkJqaihUrVqCkpMTjmq+//hoAMGTIEI/9arUajz76KJxOJ3bt\n2nXHY+rcuTOaNGni3m7ZsiUAoH379oiNjS2zPy8vz72vR48eWLFiBR566CEAgMlkQklJCRo0aAAA\nyM7O9ujrxRdfRLNmzZCeno7Dhw9j9erVGDJkCO6+++4Kj9dsNqOkpMT9LycnB7t27cIzzzwDs9mM\n559/vsw1ffr0gUaj8Wjju+++Q+fOnREaGurRXoMGDRAXF4eff/7Zff6ePXsQGRlZZpxDhw695XjP\nnTuH48ePo1evXggNDXXv12q1+H//7/9h5cqVUCgUFb7/M2fO4OzZs+jevTsaN27scaxv374ICgrC\nd999V+a6p556yqMfOSr52vepVquRm5tbZpp5zJgxWLJkSRnfPsPURXiKuBoTExODxYsXIycnBzt3\n7sSvv/6KAwcOYO/evdi7dy8WL16Mt99+Gz169ABAPk4AaNGiRZm2mjVrBgD4448/7ng8jRo18tiW\nDdH1kavyNOj1069fffUVVqxYgZMnT8JkMnkcczqdHts6nQ6vv/46hg0bhtTUVNSvX98j2rkiTJ8+\nHdOnTy+zv169enjjjTcwYMCAMseuv8fz58/Dbrdj165d6Nq16w37Ki0thcPhgMFgQHx8fJnj4eHh\nHkazPE6dOgUAZYwhAPcU9O1w7tw5ACh3PCqVCk2aNMGxY8dgsVjcU+bl9a/T6QB4vs9x48Zhzpw5\n6N+/P3r27Il77rkHPXr0wF/+8pfbHifD1FbYwNYA6tWrh6FDh7pV0JkzZ/DJJ59gxYoVmDhxIrZv\n346QkBCYTCZoNJpy/XzyH1Cz2XzH47hW2V3LrfyKALBu3TrMmDED9evXx/jx49GiRQvo9XoUFxd7\nLDW5lg4dOqBFixbIyMjAkCFDEBAQcFvjfe655/DXv/7Vva1SqRAREYGmTZveUAle34fBYAAAd0DW\njdDpdO5zb+QTvdaIlYfFYgFw4+d8uxiNxgqNx2QyeYxNNqg3Y8SIEYiNjcXKlSvxww8/4JtvvgEA\ndOrUCf/3f/9X5WuMGaY6wAa2BhIbG4tp06ahsLAQX375JQ4dOoSePXvC398fdru93GAaWTHerpGq\nLN5//32oVCp88MEHbjUNXFXd5bF27VpkZGQgLi4Oq1evxsCBA29LybVo0eJPr3MNDAwEACiVylu2\nJRsmq9Va7nGTyYSgoKAbXh8REQEAZab+7xT5XV8/WyAjf9m605+J7t27o3v37rBYLPj555+xadMm\nfPnll0hNTcX27dtr/DpqhvmzsA+2GmKz2bB48WLMnTv3pufJ05my8pGnhk+ePFnmXHk5ybW+Ul+S\nlZWFevXqeRhXADhw4MANz58/fz569uyJlStXwt/fH1OmTCkzlextmjZtCo1GgyNHjsBut5c5XlBQ\n4P5/WFgY/P39kZmZWea83NzcWxpOear99OnTZY4dPnwY69evR2FhYYXHLr/r8n4eHA4Hzp8/j0aN\nGlVIsd4MvV6Pnj17Yt68eUhNTUVhYaGHX5ph6ipsYKshWq0WO3fuxIcffoiNGzeWe05+fj42bdoE\nnU6HLl26AIA7qcLHH3/sca7NZsPnn38OnU6HXr163bRvpZJ+JG6kwu6UyMhIFBYWekxRX7p0CatW\nrQJw9UsCAEiS5PadysuFpkyZgqNHj+K///1vpY7rVuj1etx7770oLCzEF1984XEsMzMTSUlJ7rXI\nCoUCXbp0QW5uLn777TePcz/55JNb9tW0aVO0aNECe/bs8Qj6cjgcSEtLw6uvvuqe7q3Ie2rWrBkS\nEhKwe/fuMkZ/48aNMBqN6Nu37y3HdT1Hjx5Fv379yk1wIiv+irgNGKa2w1PE1ZTXX38dI0eOxKRJ\nk/DFF1/g3nvvRXh4OIxGI06fPo1NmzahqKgIs2bNcq9XTU5Oxr333ot169bBarUiMTERRqMRmzdv\nxtmzZzF9+nSEhYXdtF9ZFc+fPx9du3bFo48+Win3079/f7z33nuYMGECHnroIVy+fBkrV67EtGnT\nMHfuXPz+++9Yu3Yt7r33XuzcuRN79+7F5MmT3eN57LHH8MUXX2Dp0qXo06dPuYE73uJf//oXDhw4\ngPT0dJw9exatWrVCdnY21qxZA4VCgcGDB7vPHT16NH788UdMmDABw4cPR2RkJA4ePIhDhw6hUaNG\nZZJvXM+MGTMwevRojBgxAk899RT8/PywadMmnDhxAmlpaW5fqfxcZs6ciebNm+Opp54qt71XXnkF\nI0eOxIgRIzB06FBERUUhIyMDH330EZo0aYKxY8fe9vNo2bIldDodZs2ahRMnTqBt27ZQqVQ4ceIE\nVq9ejbi4OA/fN8PUVdjAVlPi4+OxceNGrF69Grt27cLChQthMpmg1WpRv359JCcnY9iwYe4lMQAp\nqMWLF2P58uXYuHEjtmzZAq1Wi9atW2Pp0qVlElOUx5AhQ/Djjz/ixx9/xNGjR+9I4ZTHhAkTYLPZ\nsH37dqSnpyMuLg7p6eno06cPTCYT5s2bh7feeguxsbGYP38+2rRpg9TUVI820tPT8cgjj2DKlCn4\n3//+B7XaNz++TZo0wbp167B06VJs3LgRK1euRFBQELp164bnnnvO4x1069YNS5YswdKlS7F06VL4\n+/vjr3/9K959910899xzt5zivfvuu7Fq1SosWbIECxcuhMViQVxcHBYvXuzxLv7xj3/gyJEj2LRp\nE6KiojBixIhy2+vSpQs++ugjLFmyBO+++y5MJhOio6Px97//Hc899xxCQkJu+3mo1WqsWbMG77zz\nDr799lt8/vnnsNvtaNiwIZ588kmMHTuWFSzDAFBIt/pKzTAMwzDMbcM+WIZhGIbxAmxgGYZhGMYL\nsIFlGIZhGC/ABpZhGIZhvAAbWIZhGIbxAmxgGYZhGMYLsIFlGIZhGC/ABpZhGIZhvAAbWIZhGIbx\nAmxgGYZhGMYLsIFlGIZhGC/ABpZhGIZhvAAbWIZhGIbxAmxgGYZhGMYLsIFlGIZhGC/ABpZhGIZh\nvAAbWIZhGIbxAmxgGYZhGMYLsIFlGIZhGC/ABpZhGIZhvAAbWIZhGIbxAmxgGYZhGMYLsIFlGIZh\nGC/ABpZhGIZhvAAbWIZhGIbxAmxgGYZhGMYL1CoDm5WVhdatW5d7bPXq1Xj77bd9PCKGYRimrqKu\n6gH4iuHDh1f1EBiGYZg6RI1VsA6HA9OnT0e/fv1w3333Yfz48TAYDACATz/9FA8//DB69eqFTZs2\nAQAWL16M6dOnAwCSkpKwfPlyDBw4EH/9619Z2TIMwzCVTo01sD/++COysrKwdetWbN++HS1atMDB\ngwfhcrlgt9uxceNGTJ069YbG8+DBg1i3bh02b96Mjz76CCdOnPDxHTAMwzC1mRprYMPDw3HmzBl8\n/fXXMJvNePHFF9GjRw9IkoQBAwYAAFq3bo2cnJxyrx8wYABUKhUiIiLQuXNn/Prrr74cPsMwDFPL\nqbEGtn379pgxYwZWrVqF7t27Y+LEiSgtLYVKpYKfnx8AQKlUwuVylXt9SEiIx/9LSkp8Mm6GYRim\nblBjDSwA3H///Vi1ahV27twJs9mMd999t8LXFhYWuv9fVFTkYXAZhmEY5s9SYw3sZ599hqVLlwIA\nQkND0bx5cygUigpf/9VXX8HlcuHKlSv49ddf0aVLF28NlWEYhqmD1FgD26dPHxw7dgx9+/ZF//79\ncfr0aYwcObLC18fFxeGJJ57Agw8+iJSUFMTFxXlxtAzDMExdQyFJklTVg/A1SUlJmDdvHqtWhmEY\nxmvUWAXLMAzDMNUZNrAMwzAM4wXq5BQxwzAMw3gbVrAMwzAM4wXYwDIMwzCMF/BJNZ33tx4DAFit\nVgCAJMnZleR1q5LHtlJJdl+hUECtVgEAtBoNAECtpHP99DoAgEpF5zocTnGNaElu0un0OO50OsR1\n1K5DHLc5beK40z1WOQuUWq32uEaeVZeP2+122nbStkvc3/NDk2/xZJi6gsPhRGGhqaqHwfxJwsL8\n+T3WEm7nXUZFBd1RH6xgGcYHyF8UmZoNv8fagy/epU8MrEohQaWQoFEpxD8lNColdBoVdBoV/HRa\n8U/j8S/A7+o+tUpJ/zQaqDUaKFRqKFRqQKGif0oloFTC6QJISEoAJGjUKmjUKui0avFPA51WA634\np1YroVYroVKp6Z9aC5VaC7VGB7VGC7VGC4VSDYVS7W7bJSngkhTuvuVrlCo1lCo1FEoVFEr+Rayp\n7Nu3D88//3yZ/XPmzEFmZmYVjIhhmJpInSm4zjB/Frme8J3w8MQNlTgShmH+LBvffNTrffjEwDod\n5HtVCl+rWkPdXvVtysOg47KPU/bFAoDLRb5Tu52crHYHbcv5hyWX8IsK/6dGOGFdSjqu0ZIPVyP6\nKrM6SSH6UtD1SpXK7dB1ty18ru4+3eMVbYjx8rx7zeLixYuYNGkSlEolnE4nBg0aBKPRiJdffhkZ\nGRno168fxo8fj5SUFKSlpWHbtm3IycnBpUuXkJeXh0mTJqFnz55VfRsMw1QzWMEydZ5t27bhnnvu\nwbhx43Ds2DHs3r0bZ86cwZYtW+ByudCnTx+MHz/e45rc3Fy8//77yMjIwOTJk9nAMgxTBp8YWJ1O\nD+CqAnRHCV/3qbyRMgSgFNHC8j6naEsSn2XUpVCkTnG9wuWpjl0iWtjupG2HiAC2i2hjl9MFlyRf\n4xJ9SqIPz/u7Gj0sjqPiVX2Yqqd79+4YP348SktL0a9fP3To0AEHDx501xUuLxfL3XffDQBISEhA\nbm6uT8fLMEzlcKfRwRWFFSxT54mPj8eGDRuwe/duvPXWW3j88cfd7osbIX+hYxim5pKXV1qh8+7U\nEPvEwNodQgmKbYWsIsX6UYA+1Wryk16rEBVCibpDqsUxl0use3WvmZXPFwpW7HeIXu128emgvpzX\nrYuVfbryOlhAcU19Wc/xX78O1iGudQmFyz7YmsXmzZvRuHFjJCcnIzQ0FOnp6WjWrNlNr/nll18w\nevRonDhxAg0aNPDRSBmGqUmwgmXqPE2bNsXMmTPh7+8PlUqFoUOHYu/evTe9JjAwEGPHjkV2djam\nTZvmo5EyDFOT8Emy/3e+PAjg2uhgoTJFz7JSlLMyyTJVoVRAKRTs1WuE71XoSTm7Eq5TlbKP1nXd\n7clqU/bduq9zimxMsgpVKt3jcStXl6dPVm7LcZ36lXXv5BH33fihMDWWxYsXIywsDMOHD7+t6yo6\nHcVUX6Kigvg91hJu511yJieGYRiGqUb4ZIpY9oe6A0OEKpXXrEL4U+VtlXs9qco9QndqYciRxvQp\n+1Kvj9wVwcFu1SkflRWxJL5auFW1QiPOl5Uv4BC+Y5vwFVssZo82VEJVu3W3yN7ksJpv8jSYms6E\nCROqeggMw9QAWMEyDMMwjBfwiYI1Wy0AAKeI4NVqqRKOez2sOE/2q0IoQ4Va6T5oEuoRSrW4VlS2\ncV1dtwoApaUlAK6p3CPa9hNrcXU6T6Uq9+246hAGAFgsFmSJvLN5V/Ko7ZJiOkdcGxgYAAAIDQoE\nAOh1dF8Xzp2k80Y9eLPHwjAMw9RifGJgM06cAADk5OQAACIiIgAAQUHkOA4JDaEThbWTA5f0ej0u\nXrwIAMjKzAIA6IRxDg4W1wgTarGQETcYDAAoIT8AKMR6Rlmqa1SyUZeDpGjb7vQsd2coLkJ+Ho3X\nLqZ87TbqwyEComRDaxPHzSYjnWfhclYMwzB1HZ4iZhiGYRgv4BMFe/IkTZnKhckLCwsBXF3Woven\nlHQKtQgeEgpWpVKhtJTCqO1WKoguF4GTg5pUIjmFTu/n8an186fzxJSy3W4VfVI7JnkqWahSOQBL\nXq5jMRTDUJxPnTlsog0R5CQGIQdjmUtp6thhp/PUSk6VyDAMU9dhBcswDMMwXsAnClZOxCArPodQ\nrvJ+g1CpUMsJ/eU1NpLbKSoXAlDKsUiQP+UcieK7gpCXClGWTv7UisMOC/loHXZSrpIopScrV4cI\nyLKaiuG0kk9VLUrf6dTUl1ZHfWjkJBc26sOpovP0et2tHwrDMAxTq2EFyzAMwzBewCcKVo7wlZHV\nqOL6BBRy0gj5+DUJ95Wyz1UhR/+Kou06LURj1JbkzvoPANAIVeyyUR+m0iIAgLGU/MAu4Zs1Gckn\nK0cKa1VKKEWxO3k5kVxwQBJLgsxiKZBGRCrrxHGlrGwZhmGYOgsrWIZhGIbxAj5RsHLSB1mNyrU2\n1dcl8NeIGOFr0x6qRRSwnFBC/lRqKHpYLlvncFIfUNF+uxzRqxYJ+l0iethIEb/FhZfpPBP5ZC0m\nUrBy+kNVQIC7bavwFSuUOo/xGQwiqhg0fp2W+oZNLnnHMAzD1FVYwTIMwzCMF/CJgnVerfdG22I9\nrCSXohOHXddGD4OUokuoR7g80yjK2ZMc9hKPwzahUF2IFJ+0LhYWeT0tfZotdJ3TQT5XhYv6USvo\nkTjMZnej7nJ6YpxWG51rNlCUcYAQrno/8gerdH63eCIMwzBMbYcVLMMwDMN4AZ8o2NAQSobvLnLu\njvAl6afRkDbUaUkByj5blVLpjh6WlWypgRRqaeEVAIDDSnl/5eRJKi0l9be7xLrW4FAAgDGXEveX\n5GYDAJwmkX1JEv5VuQyeiABWKpXQ62k8LrFu12omxWo0i/GJ+wmS8xvbSFVbvV7CnrkZL730EubO\nnQu9Xn/Lc5OSkrBx40YEBAT4YGQMw9QlfGJgGcaX/Pvf/67qITAMw/jGwPbo3A7A1XWvbp+m2NZo\naBg2m1hXKpSsSq12qxCjqFTz3fZtAABzEUUByxG8Gj9SIEqJ1KZd+FSNdrnMHX0kxLWg40KNFpTQ\nulh1APXjL0rQtU5o6VbUp0+eAgB3dZ0//rgAAND5UVRxTBRVBbI6qc8/8ooq/GyYP4fBYMDEiRNh\nMplgsViQlpaGF198ERs3bsSrr74KjUaDoqIi9O7dGz/88AMMBgNycnLw1FNP4fHHH3e3c+LECaSn\np0OtVkOpVGLhwoUwGAyYMmUKGjdujIyMDLRq1Qpz5sxBbm4upk+fDrvdDpVKhdmzZ6NBgwZV+BQY\nhqmOsA+WqdHk5eVh0KBBWLVqFf75z39i+fLlHsdDQkKwePFiAMDp06fxzjvvYMWKFXj77bevJjgB\nkJ+fj7S0NKxatQqdOnXCxo0bAQDHjh3DP//5T3z66af4/vvvUVJSgoULF+Lpp5/GihUrkJqaimXL\nlvnuhhmGqTH4RMHqQWtQI8IpslfONayVlaqozZp57hIAoFGTvwAA/AL8YRaVbML8SSXe06kNAOC4\nmnys4WFhAABdQDAAwOwgdWwW9WJPZ1EdWSjpj2mXrp0BAJGiFm2pgaKK1UEU+esSY/HX62EooWPW\nBvVpn5b8s1azyEwl8haHBNO1pUIVhwQHVvjZMH+OyMhILFu2DO+99x5sNhv8/f09jrdv3979/65d\nu0KtViM8PBwhISHuqk4A1ShesGABLBYLLl++jIcffhgA0KRJE0RFRQEAoqOjUVpait9++w3nzp3D\nO++8A6fTifDw8AqNNUrMdDA1G36PtQdvv0v2wTI1mhUrViAmJgbz58/HkSNHMG/ePI/jGpGQBICH\nYpUkye2qAIA5c+Zg9OjR6NmzJ9577z2YTBQ8p7ou7aUkSdBoNFi4cCGio6Nva6x5eaW3dT5T/YiK\nCuL3WEu4nXd5p4bYNwZWqNBLmX8AgPsbv8MscvyKjEl/iaD9fiILk9YE+AeQOjSLajfNGtYDADQI\n6wEAkMRaVafI+FRkomuP5+ZRGxJt14uOAADY5ShkoT6d8tpcUVVHzpucazSipJjWytrEuXZxH/Ua\nNAQA6GW16yLlqtSRT1YZzN9bfEVhYSESEhIAAN9884275nB5HDx4EE6nE8XFxTAajQgNDXUfKyoq\nQpMmTWCz2fD999/jrrvuumE7HTp0wDfffINhw4Zhz549uHLlilvxMgzDyLAPlqnRPProo/jggw/w\n9NNPo3379sjLy7ta7vA6GjZsiBdeeAGpqal48cUX3UUcAGD48OEYN24cnn/+eaSkpODzzz+HwWAo\nt53x48fj22+/xZNPPomlS5fe1BgzDFN3UUg3+mtUiTz65FgAQLhQqM2bxwKAexpOIyrkBIscxTaD\nmJ6DAlGNKDrTqaE/hvmFBQCuVsGRROSuXPfVYJXXy5LadAlF2jSc1EqYRlTfEdOFZrHGFaFira6Y\nNnTY7XDaqS2Tkf7QFlgoItkiMlA5RWRzkIL6sIk2cw00tk9XvFuxB8R4nfXr1+PUqVOYPHlylY2B\npxZrPjxFXHvwxRQxK1iGYRiG8QI+cRZq/UkdRtdvBAAICiUlq9aSf9UvgHyXcsUblVCyKidQYCbV\nqJBEsIqG1qvaLKQS/bV0rZ8ftaVQkpoM0It8wHb6DAui6FKNWDdrEkrX6qDtqEhSuHKd2eysbDhB\n50TVo2CWEBEwYxBuPksJ+XP+bFPuAAAgAElEQVRD1WLNrfDVukrNFX00jI8YOHBgVQ+BYZg6BitY\nhmEYhvECPlGwDZo1AwCExZASVIu1iqFi7apeqEtZMSJMSESnCwbhp4WKfKOBYtmENjqGtkU2Ja3I\nBlVQRFmUbE45OtghmqI2VSLvsVpED5vzKKdxoZXUs91G/tYSq9mdW1gvFKrSJqKE7fS9JEgtqgEp\nRP5iBSncUD/+3sIwDFPXYUvAMAzDMF7AJwrWIGq3Hs04DgDwE/5Rfx35ZvUBpGC1elKAcg5gtUrt\nThTg50e+V72WtgPEmlOt2JaDoYNENR1ZVQqhCkmso9UKBesSa2+VOlLRdom+a9hdFFUWU18HmChi\nGSZSuS5RQ7bkMvleFWqRiUr0qRL5kOuHBFf42TAMwzC1E1awDMMwDOMFfBNFLFSoU+QJFiISTgf5\nV4sLSBnaXRQZbLeRP9Rms7kVrEpDitWmIXWo9SP1KytbOa+xXNczWNRyDbiu1qxeFI6Vx6QPpZzF\nASJ7VLBOfOdw2OHSkCq2izzGOlFxx6qjthRCJZtERLNfoHicyhtnE2IYhmHqBqxgGYZhGMYL+ETB\nBrh9rqQeoyKoAk79mHoe59mdpCLlfMB2uw1mUbnGLLIoyVHFZmsufZbQ8SsGo8e1JgUpXLuaMnCo\nhHL1E8pWLxRtkFDA4XqhfEWVHY1KAa2a9in86DH5yetzw8hnXFpMEctmJ/ltC3Pp02FhBcswDFPX\n8YmBNZvIOBYUXAQAXBRJ/1V3dQQANGpE5emCgzyTPaiUSjhF+kGFSBChcpIhdYpUiU6R6tAsDKvN\nSvuLjfRZVEIG2WQsFWOh60uLsun4JbrulEsksHCSIXY6XFCpxDSyML5+LlEwXizpKcqjou8QY1GJ\nwCmnzVThZ8MwDMPUTniKmGEYhmG8gE8U7OV8WuYiq0ulUKNHj2cAADKzSAnqdaQUQ0JE4FFAgDt4\nyV8EL2nFdLNGBDupdCLxhKizHSCmcaNFmTqlg9SzSySccAi16RCBVHaxfKdYLCUqLqYi3CZjKcwi\n9aHFScFOF7JJgWcXkPrV+tF9hITTmGJEMQOF8WqdUYZhGKZuwgqWYRiGYbyATxSsRgQWSRDqU6Q7\nVKpJdtocYjmMk9IUlpqu1uFUiXMVooyc7J8NFMFIgYFC2crLcITC1YmlNDqRiEIrkkIoRbk6hYbU\np59IseivoO2YaAq8UricsNpI3ZrsNL7YppRgIjvzHO0XBdnFah4ohL+4pNRSsQfDMAzD1FpYwTIM\nAKPRiKSkpKoeBsMwtQifKFi9XEpOdk26SPK5hH03C9+sXiR5kJNFOBwOOBykIl0Op/ikJortogh6\nntghVKS71J1QqgGBpHTlFIvyGLRqlThflMFTUiILnVDM/jo1tCK5hUYrLzOiMWgk6sNUYhbX0HZR\nPqVWPH/mfEUeC8MwDFOL8YmBZZjqiMFgwIQJE2C1WtG5c2cAwIEDB/DWW29BrVajfv36ePXVV6HV\navHvf/8bBw4cgNPpxPDhw/HQQw9hypQp0Gg0KCoqwuLFi6v4bhiGqW74xMCqFaIbNSlChTsDP304\n7aQMJfpwq1W4JChBklOhEGtQIY6J5P7yp80mRw2TovUTUtVmpnWvxmI6brVZRTuEXe5LSWPUyykU\ntRpohDKV0yoaDKRQC/IpyUWJgXyydpEqsaiAEk9YRMEApnqzYcMGxMXFYdq0afjqq6+wefNmzJ49\nGx9++CFCQ0Mxb948bN26FQ0aNEB2djbWrFkDm82Gxx57DMnJyQAo4v3VV1+t4jthGKY6wgqWqbOc\nOXMGXbt2BQB069YNV65cQWFhISZMmAAAMJlMCAsLQ05ODg4dOoSUlBQAgMvlQl5eHgCgffv2Fe4v\nKiqoku+AqQr4PdYevP0ufWJgnU45G5OsRuVPOq4UkbySiMI1GSkTksvlunquaEshon3lKGGlCNOS\no4tlHELJStcpXXh+QCfUqUuM0S6vfTUZYTGTj9Uk0jMaSovEWGSF7RLn0nkKFY2pQcMGN3scTDVB\nkiQoxQ+Qy+WCRqNBZGQkVq1a5XHehx9+iCeeeALPPPNMmTbkYhQVIS+PZzZqOlFRQfweawm38y7v\n1BBzFDFTZ2nWrBmOHj0KANi3b587wcnp06cBAKtWrcKJEyfQvn177Ny5Ey6XC1arlaeEGYapED5S\nsKT41Cqy526rLlSlUkT+KoSudAcbC4VIp3r6baHwVMPX9yV/Kq47T1a0snJRyMpW+IHlsel0Onfi\n/7wCykSlFUrbLqKe7SInsd1CY2vcqDEAIFCs+2WqNwMGDMC4ceOQmprqDnKaM2cOpk6dCo1Gg+jo\naAwePBharRaJiYkYPHgwJEnCsGHDqnjkDMPUBBSSew7Vezw1fT6Aaw2sbCyFsRM2smIGlgyhTk8B\nU/K0shzkJBtWmVsZWJVsoO2e12l0WqhF8grZwNpFZZ/rDayhhBJOXG9gl7z9RnmPg6mj8NRizYen\niGsPvpgi9o2CFf5QlShQLsm+V/kEYfTcRvRav6v7/3S2JIyzXJRdNpiyMZYNrrytkJWqovz8wBZh\nNEsuU9CKXBbPLzAA0Q0oq5NDRB43bx4LANAoye9WXEh5i6+Ia4NENaB6kdE3fhgMwzBMnYB9sAzD\nMAzjBXwyRcwwDMMwdQ1WsAzDMAzjBdjAMgzDMIwXYAPLMAzDMF6ADSzDMAzDeAE2sAzDMAzjBdjA\nMgzDMIwXYAPLMJXIa6+9hsGDB2PIkCE4fPiwx7GffvoJTzzxBAYPHoylS5dW0QiZinKzd5mUlIRh\nw4YhJSUFKSkpyM3NraJRMhXh5MmTSE5OxurVq8sc8+rvpcQwTKWwb98+acyYMZIkSdLp06elv//9\n7x7H+/fvL128eFFyOp3S0KFDpVOnTlXFMJkKcKt32bt3b8lgMFTF0JjbxGg0SsOHD5dmzJghrVq1\nqsxxb/5esoJlmEpiz5497kLssbGxKC4uhsFgAABkZmYiJCQE9evXh1KpRK9evbBnz56qHC5zE272\nLpmahVarxfLlyxEdXTaFrbd/L9nAMkwlceXKFYSFhbm3w8PD3YXZ8/LyEB4eXu4xpvpxs3cpM3Pm\nTAwdOhQLFiy4WneaqXao1Wro9fpyj3n795INLMN4Cf6jW3u4/l0+//zzmDp1KlatWoVTp05h27Zt\nVTQypjrDBpZhKono6GhcuXLFvX358mVERUWVeyw3N7fcKSumenCzdwlQLeGIiAio1Wr07NkTJ0+e\nrIphMn8Sb/9esoFlmEqie/fubiVz7NgxREdHIzAwEADQqFEjGAwGZGVlweFwYOfOnejevXtVDpe5\nCTd7l6WlpRg1apS7BvX+/fsRFxdXZWNl7hxv/15yNR2GqUQWLFiAAwcOQKFQYObMmTh+/DiCgoJw\n3333Yf/+/ViwYAEAoG/fvhg1alQVj5a5GTd7lytWrMAXX3wBnU6H1q1bIy0t7YY1p5mq5ejRo3jj\njTeQnZ0NtVqNmJgYJCUloVGjRl7/vWQDyzAMwzBegKeIGYZhGMYLsIFlGIZhGC/ABpZhGIZhvECN\nMLCHDh3CiRMnvN7Pvn37cN9995V77M0338TatWsBAAkJCcjJyfH6eBiGYZiai7qqB1ARPvvsM3Tu\n3BktW7assjFMnDixyvpmGIZhah5VYmDXrVuH999/H06nE1FRUZg3bx7Wr1+PnJwczJkzBwCwePFi\n5OTkoG3bttiwYQN27NiBgoICpKamYuHChe41anfddRdeeeUV+Pv7IyUlBX/729/w7bff4vz585gw\nYQKKi4vx5ZdfQqlU4j//+Q8aN26MixcvIi0tDVlZWdBoNPjHP/6BAQMGuMf3xhtvYMeOHVAoFHjt\ntdfQqVMnTJkyBU2aNMFzzz3ncS+ffPIJPvjgA9hsNtx111147bXXbpiWi2EYhqk7+HyKOD8/H7Nm\nzcIHH3yA7du3o0mTJli2bNkNzx86dCjat2+PSZMmYeTIkdiyZQt27dqF9evXY/PmzSgpKcGHH37o\nPn///v1Ys2YN5s6di/nz56NevXrYunUrWrRogc8++wwAkJaWhm7dumHbtm34z3/+g9mzZyMrKwsA\nkJ2djbZt22Lbtm14+umnMWvWrBuO7cCBA1i4cCFWrFiBHTt2IDAwEAsXLqycB8UwDMPUaHxuYCMi\nIvDLL7+gXr16AIAuXbogMzOzwtd/9913GDBgAPz9/aFSqTBw4EDs3r3bfbx3795Qq9WIj4+H2WxG\nv379AADx8fG4fPky7HY7fvrpJwwbNgwA0LBhQyQmJmLv3r0AAJ1Oh/79+wMA+vfvj99//x1Wq7Xc\nsezYsQMPPPAAYmJiANCXge3bt9/mE2EYhmFqIz6fInY6nVi0aBF27NgBp9MJo9GIZs2aVfj6goIC\nhISEuLdDQkKQn5/v3g4ICAAAqFQqj22lUgmXy4WioiJIkoSgoCD3NcHBwSgoKEDjxo0RGhoKpZK+\nd8ip0YqLi8sdS2lpKb7++mv8+OOPACghuN1ur/C9MAzDMLUXnxvYr776Cjt27MDq1asRHh6O//3v\nf9i4caPbAMrcyKhFRkaiqKjIvV1UVITIyMgK9x8WFgalUoni4mK3oS4qKkJERESZfktKSgAAoaGh\n5bYVHR2Nxx57DJMnT65w/wzDMEzdoEp8sA0bNkR4eDgKCwuxZcsWGI1GREdH4+TJk3C5XCgoKMCu\nXbvc16jVapSWlgIA7r33Xnz55Zcwm81wOBz49NNP0atXrwr3r1ar0aNHD3zyyScAgAsXLuDAgQO4\n5557AAAWiwVff/01AGDbtm1o164dtFptuW0lJSVh+/btKCgoAAB88803+O9//3v7D4VhGIapdfhc\nwT700EPYvHkz7rvvPjRu3Bgvvvginn32WZw6dQr+/v5ITk5G8+bNcf/997unfpOTkzF//nxkZmZi\nypQpyMjIwMCBAyFJEhITEzFixIjbGkN6ejpmzJiB9evXQ6PRYPbs2ahfvz4uXLiA5s2b47fffsOb\nb74JpVKJ119//YbttGnTBmPHjkVKSgpcLhciIiKQnp7+p54PwzAMUzvgZP8MwzAM4wVqRCYnhmEY\nhqlpsIFlGIZhGC/ABpZhGIZhvAAbWIZhGIbxAj6JIu58V3cAgEKhAABER1PmI5OR1pmaDbT2VBlI\na1HvffgxAMChHT8iVEGJIroPexwAsGcXLaGJL7UBALKK8wAAOzMpsUReAbXlr7oMANBpnDQIhQYA\nICnoPKdkAAC4YKZtB8V6OeyUtckluaALiAIABAfRuEryrgAAiq9cAAAEhlKyioBwOp6ZdZb6ctLY\nHKaSij0gptbjcDhRWGiq6mEwlURYmD+/z1rGzd5pVFRQuftvBStYhvEBarWqqofAVCL8Pmsf3nin\nPlGwSgV1I2dqunKF1rcqQdsqSdh5F306XXSjmoAwOJ2kPJ0qqlCj8KPsS/UaUS5jc242Hb9wHgCg\n11B6w4b1SPm2atkEAKAWySJKjKRwz5yl+rL5BZdF3xD9kMo2GEvg0DoAAMHiWrs4yWqjbznBTj8A\ngFZ8T1G76D5VQqkzDMMwdZcaUQ+WYa5l27Zt7iIOlcW+ffuwZs0aLFq0yGP/nDlzMGLECHzxxRcI\nCwvD8OHD76j9hyduqIxhMgxTSbw/JcnrffjEwLpc5N+Uc1pYLeTn1Ai16K8klepwyMqPtsPqNYKf\ni/bptaQSlQpqI99mobZEUv+wMH8AgEpBqvOeezoAAJ74+wMAgOxLuQAArV4HAPj2228AAFu2bAEA\nqEWCf6eC1KpSo4ISpIahovl3tR/5WqGmbaeS+nQp6Tyny0+MgXN3eIusrCxs3ry50g3sjZg+fbpP\n+mEYpvbBCpapFhgMBkycOBEmkwkWiwVpaWl48cUXsXHjRgQEBOCNN95AXFwctm7disOHD2PJkiVI\nTU3FlClTUFJSAofDgRkzZqBNmzZITk5GUlIS9uzZg7/97W+QJAm7d+9Gz5498fLLLyMjIwOzZs2C\nUqlEQECAOx1mcXExxo0bh+zsbNx3330YN24cUlJSkJaW5jHWf//73zhw4ACcTieGDx+Ohx56qCoe\nGcMw1RyfGFiFQqhPpfxJqlMJUnoqFalGtZYUoU5L/taYRg2hsVC0b0wwDbVl0wYAAINQpLoAUrtd\nE+MAADYzKdu/9eoMAKjfKJzGoCclrFFRH1273Q0AyM4mH6zVSpG/NieNsdSqgsVB4/Tzo2sC/ahq\nT6A/bWu01LdL+IdDKTgafvy15bbJy8vDoEGDkJycjD179mD58uXlnjdq1CisWbMG48ePx5IlS9Ch\nQweMGTMGR44cwdy5c7F69WpkZWVh8ODBeOmll9CtWzesXr0aL7zwAnr37o2XX34Zc+bMwb/+9S90\n6NAB7733HlauXInExERkZGTg22+/hUajwf33348nn3yyTP8HDhxAdnY21qxZA5vNhsceewzJycnQ\n6/XefkQMw9Qw2BQw1YLIyEgsW7YM7733Hmw2G/zFl5ibcfToUTz77LMAgHbt2uH8eQp0CwwMRGxs\nLADA398fbdq0gVqtdgfZnTlzBh06kAshMTERS5YsQWJiItq2beuuHxwbG4vMzMwyff766684dOgQ\nUlJSAFDgXl5eHho3bvwnnwDDML6kvKU3d7oc50b4xMBKIkLX4ST/qEZE5boVrYa+/UsiGtcu1qLG\nt4pDrLjhZk0parhtAhVnd4g/lhJIZZodtN5VBVKqIQHkaxXiGDH1SX06bHTLbdu1AwCEhoYBAAqL\naI3rlQKKEM7KteL8xUIxHlK3AcIPrG5JalkpVMuJk6cBAE1bxAMAGkeHVfjZMMSKFSsQExOD+fPn\n48iRI5g3b57H8fIK2SsUClxbq0I2oCqVZ7i9Wn3jH3O73e7+OVRcF/19/TYAaLVaPPHEE3jmmWdu\ncUcMw1Rn8vJKPbajooLK7Lv22J3A62CZakFhYSGaNKElVd988w3sdjsCAwORl5cHp9OJQ4cOAaAv\nZQ4HfVFr164d9u3bBwA4ePAg4uLiKtRXXFwcfvvtNwDA/v370bZtWwDA8ePHYTabYbVacebMGfd4\nrqV9+/bYuXMnXC4XrFYrXn311T934wzD1Fp8GkWskLMoOWnbKRSHVk12XiuUh1JB+1u0aIAGoRSh\nm1dA3ywsNjp22UwZmALFlF7jaDovUKjjonzy3eaI7EsZZ88BAEwGUkJ2G42htJTWxdoc9KnRUXv+\n/qFQqamPEyeO03jNNAa9jqKFI+rTtKBVZGy6bMwBAKitlTvNUBd49NFHMXnyZGzduhVPPvkkNm3a\nhGeeeQZjx45Fs2bN0KJFCwA0dXv8+HG89tpreP755zFt2jSMGDECkiThlVdeqVBfM2bMQHp6OhQK\nBUJCQjB37lwcO3YMrVu3xrRp0/DHH39gyJAhCA4OLnNtp06dkJiYiMGDB0OSJAwbNqxSnwPDMLUH\nn9SD7di2l2enYkoOYgouUE/GUasnw9Ttgb4AgH4P9HYb2OJC3xpYpSYUJ/+4BAA4dvTmBja/gNI1\nqiWaXm4QQfex+dPFt344TJ3hRtNPTM3jZtOJTM3EG1PEvvHBikxNag1F3SpE9LAkXFwuEVUsJ3QK\nDiL/af2YcJw9kynaIMNpspGBPHye8gErRCN5DSjDU+eWFNxyMZsMa2ERGeLdP9KUYEYG5QvWaslI\n2u0UdXw5/w8xJhqbS9LD4iBjXi+GwoONFmrz4vkCAIBBzqEsXH6Zpw4CAI5Y8sWds4FlGIapq7AP\nlmEYhmG8gG9yEStJkUoi17BCTSpRL7IqqYWajGlCkb5t2zQHAFy8lI+LeaRA/YNI/RYbTKJNaksl\n1qJeNlEUsVlEEet0pHgb1gsFAHS/uxsA4My5LACA0SraDRRrXFGf9peSP9VhtkIlZs+bNaGp4Ct+\n1Fd+HrVhs5GSDdCTelZIpK5dDsNtPB2GYRimNuITAxsQSMEiNjtFf/qL5BARkWT8NDoycr2S7gEA\nxNSLBgD8+NNxaIMpsYTZQtO1hQZawqMS/tqAALrWaiKjZhFTyCpxZ1aReCK+FUWYtoinKeT9v9B0\nrg7kcw0NIwPrdIpALHsR4CSjff48GdRA0Zd/IF1jFOX2/IWBlRNnyEFaDMMwTN2Fp4gZhmEYxgv4\nRMHqAyjCVyOW5TRsRFPBHTokAADCIimJfpe7Kb3hkd9PAgDOnj6D5h1JWdpEQfQCIynYYDG1C4nU\nsJ9Ix6hw0HEIFVlQQlFhMRGkhP8qpoozTp0BAJhNdL5fACWH0GpJnZqURihFEguDgaaTHWJ5UcPG\nNIV9OVdED2voMWrkhAZK/t7CMAxT12FLwDAMwzBewCcKtsREqQZlH2WbNpQ5Z+iTjwEAdAGUctAm\nUinm/7CHtq0W2C1GAIDdRcccwu+pD6AgphCNKGenoUApP1GVXqknZSsXWPc3koqOi6NUi+3atgIA\n/HyAMgQ5bdS+n47GYtFp4HSSCtaI5UWSi3yyfgEifWMs+ZBD/En1Fl+kIu4WOyd+ZxiGqeuwgmUY\nhmEYL+ATBSvcptCqyJ5HRJBfVaMhVemO/BVK8Z67KZq4fv2LcKpImRqs1IgVtEwnXE3XtGxI5egC\nxbKc4CBStAbhk7U7yX+ae4kqrYQHk783vnlDAMD5c+SL9QugR+EniqoH+zthF+OSk77bRNJ/uygY\n36w5RSS3FGn8sk6SGs63FVX84TAMwzC1ElawDMMwDOMFfJMqUUlKVa0he67R0rrYokJShBbQp8gR\nAb2ejid2rQ+rUK7FJeRLjdILv6jwteoV5DtViFSKdlFpJSCIlG9UJPlLjSZSlXYjpTFsUo/2P3J/\nd2rHX6yrFdHJVosZDlECzyHWw9pEUXaTiDxWikLr4REUFd267V0AgKN2c4WfDcMwDFM7YQXLMAzD\nMF7AN+tg/WiNqb8fdacXmY9sdpKsDgX5Xs0m8q/abJR9SQUtNMJvGxZAqjY4ntbOSkqKSDZbSFWW\nGujTCVKbDYRvNrFbB+rDSpmeNBIpX4XI0K8Sa1iVmuv2K9WQJbVcXk+uQyrqA8Bopf0lQtGGCPXb\nrGnZOqIMwzBM3YIVLMMwDMN4AZ8oWLVYo+ofTOtF1X60LWdnUutINfr70XGNivypNqsEp1CwJpGD\nuNRMSjQ0glSwSkPX6PxUoi/y0TqddGshoXSeRiH6dsnfKUTJPCUpXknOHyzXzJPUkESheLlkrktc\n69SRsvVz0GdACPliIyI7AQBat4qt4JNhqopt27ahX79+d3RtSkoK0tLSEB8fX8mjYhimNuETA8sw\n1YmsrCxs3rz5jg3snfDwxA0e2+9PSfJZ3wzDVA0+MbAKBanE4HCK1FUKxVpiosjg0ktU9i0muh4d\nV8nF0JVwSXIxdr1oS1TVySdF63BSpqeIcFq/qnHStaWiML0klKpeS+pTzlmsVMnK1SkGSR+yvlUo\nJHdBeNEEJCXtkCv1qHR0QOui/VahaHVCVTPVk1mzZuHw4cNo2bIlHnnkEWRlZWHChAlYu3YtFi1a\nBABITEzEvn37cPz4caSnp0OhUKBjx46YPHmyux2DwYCRI0fitddeQ1xcXFXdDsMw1RT2wTJ1jlGj\nRqFbt24YN24c7HY7PvroI3d94euZPXs20tPT8fHHHyM/Px/Z2dkAyG0wefJkjB8/no0rwzDl4hMF\nazKQQg0OJt+kRkPK7+wZqpqz49vvAQB9+twHAGjVmnIVO50OqITfUyVko5+eFGpubi4AwCXyA4eF\nkK/VYSfFa5ZdqrL6DJALtNP5KlBEsFIl13Cl6yCyNklwQOn+vzgkPpVC2qogj01sq2T/LX9vqSm0\nb9/+psfPnTuHli1bAgDmzZvn3r906VLUr18fvXr1uqN+o6KC7ug6pvrA77D2UdnvlH2wTJ1GLuQg\np8OUkZdk3UjZBgcHY/fu3SgsLERYWNht95uXV3rb1zDVh6ioIH6HtYybvdM7Nbw+MbD5eRcAABYj\nKVitiuTlyYzfAQCHj5CSrdegKQCgYaPGAACNRgmVipSlRk2RxRYzZUkqLLgCAND76QAAKjkaWK66\n46Q/jBYhP7Vq+gPq0olBSSJLk8jWpFLSH1qN6A+S66oP9jpkBauQTxB9KIXGVd7oQqZaoFQq3QZU\nJjAwEJcvXwYAnDhxAkYj+fZjY2Nx6NAhdOjQAdOmTcOoUaMAACNGjEDHjh0xe/ZsvPnmm769AYZh\nagQ8l8nUOWJjY3H8+HGUll79ttqyZUv4+/tjyJAh2LBhAxo2pGIQ06dPx+uvv46hQ4ciJCQEsbFX\nl2A9/vjjKC4uxrfffuvze2AYpvqjkORFnl6kTadhAICENpSFadjgRwAA27ZsAQCczaQ/dAktqSpN\n0r1UTScsNAAuu5iqU5DYlqOCi4upLqxGZGBq0CBa9EbnOR303UGrE5malJR+yWIn322TxtHiuE6c\nT+04bKRozYZSt+9VJbI7yVI1UKhmeXrx8hXKb2w0Ux9FJZSRqnf3u279cJg6A08p1h54irj24Y0p\nYlawDMMwDOMFfOKDDY+gNapWUV/1yBHyvRYWUVam2JakbLV6UoRGE/m/oiKCYHfI2ZRknxltBwdR\n3l+VCEIxiwxPKiX5al1OUp1KBW3v+3kXAOCXgz8BAB54MBkA0Ls3RYGaTJTL+GIm+eGcdgnXi3u7\nqAcb6E9tB4mKPbv37gcAXL5C0dIWK6lgVrAMwzB1F1awDMMwDOMFfKJgg0KoXqpLRNkeOXYGAJBf\nQH7Upm1pDatT1FEtKCgEADRpEONeiyqvlnDJ61sh/iMUrd1K6tgGqziR1HBJSR4A4JtvtgMAzpyl\niGa9nubU24oarloNKWK7yMakVfvBJYm8xiLS2CnqwhYUka/VLrJMnc+kiOYzf1DbUPDqJ4ZhmLoO\nK1iGYRiG8QI+kVphkTEAALPwwZpLiwAAJispRJXIE1wkfLK/nyBfZr3ISAQHUA5irVasPRVLTBVl\nlppSW/JulZqU7blz59mVCDgAACAASURBVAAAf5w7K64LBQCcPnURAHDk8CkAQIcOVDdWpyOfrUoJ\nqMXaW9kVqxRtGkzUl1VEHjtBarnURPcniZq0DMMwTN2FFSzDMAzDeAGfKNgmTaIAACYLKTuLiVSk\nnLJXJZSiUWRpOv7bIQBAcYEJ9WPo2kaijXr1wgEAQaIGq0rkEtYIJ60KpCr1GlqrWlpIqthppf0x\nzShLlNVOa1WPHCEF26wpJWyXRG5jpRLQqGTVLHINCzWt09N4zRZSrH9pTG1qtXTcKvzBDMMwTN2F\nFSzDMAzDeAGfKNhGDUixGozCd2kTkcEq8l06zKRCCy9TRqSLWecBAJaSYpwJIsUa/DslVI+IpIjj\nhg0pE1OTvzQAAMREkcIN9KfvDApR7zUrh9rU+NEY2nfuCAD4/fejAIDsSxRl7BCiUyXWzaoUgFJ8\n/5B9sGqlyAol14MVFX5atYynsTRuAgBwOuQIZ4ZhGKau4hMD67BRkghjKU3LFpfSUpr8yxTMVHiJ\ngp7yzmcBAEyFFIAEy0WUFNFymkuXyLAGB5GhzTpDySt+D/kDABAaSUuB6jWk/bEtGgEA/sgnA6oK\npqQQUTFkmHNyqJ2iQkqN5RLFAfz9aZpXrZGg1opKK/CsvO4Uy3d0fjTtrFLTdHRWNiXQMBpM4vzW\nt344DMMwTK2Ep4gZhmEYxgv4JiOCu7yb2BQJG6wiqCnnAiXgv5SVDQCw2Wi/w2WFwkRJJxQieUNp\nISnMPG0AAMDfn5RoYAhVPzl3LhgAcPgoTfVmXSI1HFOPjsvreyIiSPGevEipEYsKKT2jXDzdZbdA\nbRUFBq5LmegSS4JcLtpvMlLiiSv5pMgNBmNFngrDMAxTi2EFyzAMwzBewCcKVi3KvQUI/6bIPIgG\njcgf6nKJNIcK8pvqQkXCfkmCpZTSEFqMpDRtVlp2Y7GRsjWaSPUWF10CAOgvk882IJR8tlYH3WJg\nEyooINegs1opcX9+AQVBFRbSZ0AQjdVkK4JTLNk5fuw4AOBSDvUhOV0ebZjN5FM2Giximz4xcVBF\nHg/DMAxTC2EFyzAMwzBewCcKVqsRS2cCSJn6+VNEb3gUqczI+vTZpKgeAKCoiHywJaUuGPMowrgw\nh5bu5OVTQn2jmRStxUpK1mqh8+zWIrFNhQTUalrmo7SRdHWJagHFxXTelXzy//76214AgNnaHADg\nhAVKBY27tESoXHGuobRU9EXK2+GQfbLkv5WkMnkcmRrEvn37sGbNGixatMhj/5w5czBixAg0FolF\nGIZhbgaXfWGYCjJ9+vSqHgLDMDUI3/hgRTpDh5PWw6qUpPj8NdS9Xk+Rv2EhVDKuOEz2jzpgCSO1\nUBxBSRxC8ykquNRIvtniIlKVpUWkbE2lpGztoi+Hg9SmzSoUrUhuIZe7swilu3nLpwCA73eJta8K\nlzvxv1xCQHLXynOJNknB2sU6X5tNLmvHiSZqEhcvXsSkSZOgVCrhdDoxaNAgGI1GvPzyy8jIyEC/\nfv0wfvx4pKSkIC0tDdu2bUNOTg4uXbqEvLw8TJo0CT179qzq22AYpprBCpap82zbtg333HMPxo0b\nh2PHjmH37t04c+YMtmzZApfLhT59+mD8+PEe1+Tm5uL9999HRkYGJk+ezAaWYZgy+MTAhoSQz1Vv\nJ3VoE+tg7Vb6VIiybzo1+Tx1IaQc1ZIThRpSsy7hx7VrAgEAUhGpYqWeVG9QGK1zNZtlZUs+29Li\nHABATiFlicrMzAQAXMym6GOrjRTu5TyKTr5ymfqD04nrlr9CDkFWiv2yP1cEG7vL28lJ/5maQffu\n3TF+/HiUlpaiX79+6NChAw4ePAg/P/q5vX4dNADcfffdAICEhATk5uZWqJ+oqKDKGzRT5fD7rH1U\n9jtlBcvUeeLj47Fhwwbs3r0bb731Fh5//HGo1Tf/1ZC/XN0OeXmldzpEppoRFRXE77OWcbN3eqeG\n10c+WFKosg9To6Zts4vWjzqEz1LOkKSSyLcZHaZHqChLZ7JR5iZLPYo4ltecXrlSDADIK6T8v04X\nFXc3GikiOTeHfLNFBnpwJ45Tkv/L2bTfYab9Shd9Op0ij7DkhMulEndACkaSHGJTK84V4xbjVbiL\nA/Dqp5rE5s2b0bhxYyQnJyM0NBTp6elo1qzZTa/55ZdfMHr0aJw4cQINGjTw0UgZhqlJsIJl6jxN\nmzbFzJkz4e/vD5VKhaFDh2Lv3r03vSYwMBBjx45FdnY2pk2b5qORMgxTk1BI5TmYKpmP/7cTAOBw\nkALUaEQkr1gvajCSklWIjE9qtcp9rcVG6tBoonPkKu1Op8gDLPYXGoVfV6xFlafwTCZSpJkXzgEA\nzpyiYu4l+eSDdVoofzCcpGAlFyljCQBcNE45ibLTJfyzklDkbmVr8ehTVr5OodCZ2sXixYsRFhaG\n4cOH39Z1PKVYe+Ap4tqHN6aIeS6TYRiGYbyATxQswzAMw9Q1WMEyDMMwjBdgA8swDMMwXoANLMMw\nDMN4ATawDMMwDOMF2MAyDMMwjBdgA8swDMMwXoANLMNUIq+99hoGDx6MIUOG4PDhwx7HfvrpJzzx\nxBMYPHgwli5dWkUjZG6Xm73TpKQkDBs2DCkpKUhJSalw4Qemajl58iSSk5OxevXqMscq9fdUYhim\nUti3b580ZswYSZIk6fTp09Lf//53j+P9+/eXLl68KDmdTmno0KHSqVOnqmKYzG1wq3fau3dvyWAw\nVMXQmDvEaDRKw4cPl2bMmCGtWrWqzPHK/D1lBcswlcSePXuQnJwMAIiNjUVxcTEMBiqDmJmZiZCQ\nENSvXx9KpRK9evXCnj17qnK4TAW42TtlaiZarRbLly9HdHR0mWOV/XvKBpZhKokrV64gLCzMvR0e\nHo68vDwAQF5eHsLDw8s9xlRfbvZOZWbOnImhQ4diwYIF5dYOZqoXarUaen35Nbsr+/eUDSzDeAn+\nY1v7uP6dPv/885g6dSpWrVqFU6dOYdu2bVU0MqY6wgaWYSqJ6OhoXLlyxb19+fJlREVFlXssNze3\n3Ckqpnpxs3cKAAMGDEBERATUajV69uyJkydPVsUwmUqisn9P2cAyTCXRvXt3t4I5duwYoqOjERgY\nCABo1KgRDAYDsrKy4HA4sHPnTnTv3r0qh8tUgJu909LSUowaNQo2G5Wx3L9/P+Li4qpsrMyfp7J/\nT7maDsNUIgsWLMCBAwegUCgwc+ZMHD9+HEFBQbjvvvuwf/9+LFiwAADQt29fjBo1qopHy1SEm73T\nFStW4IsvvoBOp0Pr1q2RlpYGhUJR1UNmbsLRo0fxxhtvIDs7G2q1GjH/v70zj5KyPvP99629q7u6\neqtuaJqt2QQRUQSTMIISEHFi9DoaBEHO1auTM4rLmFw1E0JGRx0d0DPxxNwTb2KUaJZjOHq8ZNLR\no8lExQ5EWVVodrqh97325b1/PM9bsYVgg11VvXw/f/jyrr+n3tfqX33fZ6uowKJFi1BVVTXg31NO\nsIQQQkgG4CtiQgghJANwgiWEEEIyACdYQgghJAMM6Qm2trYWS5YsOWX7xo0b8Ytf/KJf11iyZAlq\na2sH2jRCCCEjHEeuDcgE999/f65NIIQQMsIZFhPsE088gbfeeguGYeCxxx7Dr3/9a4wbNw7/9E//\nhEWLFuH666/H66+/jueffx7t7e144IEHkEgksHDhwlybTgghZJgypF8RA0BDQwNmzpyJmpoa3Hrr\nrXj44YdPOaapqQk1NTWorKzE97//fdxyyy2oqanBRRddhPr6+hxYTQghZLgz5CdYt9uNZcuWAQCW\nLVuGjz/+GNFotM8xl19+OQAgGo1i9+7duPrqqwEAV111FfLy8rJqLyGEkJHBkJ9gi4qKYLPJx7BK\nmHV3d/c5xu/3AwA6Ozv7HGcYBgoLC7NlKiGEkBHEkJ9gu7q60v+2JlZrQv0s1narn2MqlepzPiGE\nEDJQDPkJNhKJ4I033gAA1NTU4IILLoDL5TrtsR6PB+edd176+C1btpzyOpkQQggZCIb8BFtdXY0P\nP/wQV111FX72s5/he9/73hmP//73v4/nnnsOS5cuxa5duzBp0qQsWUoIIWQkwWL/hBBCSAYY8gqW\nEEIIGYxwgiWEEEIyACdYQgghJANwgiWEEEIyACdYQgghJANkpdj//3lGUmdSKQlYttlk2DyPFwDg\n9foAAE6XBwBg6P6kmUTKlDxV00iKwQ7JcfU48wEAduvYRBwAkIjrUtdjsRAAIBoN9Rnb5ZKxYci6\n0ylLj0dssDvsgJmQsVOyTCV1HaZ+nmTfpdl3/fpv3NPPO0SGO4lEEh0doVybQTJEcbGXz3cYEwj4\nzuk8KlhCsoDDYc+1CSSD8PmS05EVBetwido0DJnPLRVpVzVq2mSZTMn2VFwUYDwRRjwRlG2IybX0\nnKhDlK3L7tRri6o0k7rU9F6bbnfY5QtgWDbZ+9pi7be+J04HYKi6NUxVyUnLLhk7pddO2fSqNjd0\n8P7dGEIIIcOWYdEPlpCBpLa2Fi+99BJ+8IMf9Nn+6KOP4pZbbsHYsWPP+prX3P/aQJlHCPmC/PTB\nRVkZJysTbIG2hHM4RX06VHU6dd1QVarCFam4pS5NOFVRJk3LPysbzJRsT6TE12oYutS33gbkOLuq\nS7vH2Xe73dTrRfvY8lcla4fNpufYRZnmqbw1dXDLzxvXZUp9s1Sww5N/+Zd/ybUJhJAhBBUsGfGc\nOHEC3/72t2Gz2ZBMJnHjjTciGAziW9/6Fvbt24elS5firrvuwurVq7Fu3TrU1NSgsbERJ0+eREtL\nC7797W9jwYIFuf4YhJBBRlYmWK9XFKxhiJq06dJuWFG34l+1wgQMW1LXUzDVR5owRZma6WNUgaZF\no6HXEnWZ1Ihem6rJtJLV88xEWI5LyTIeke1RpyjlPK8Pnjy/niPqNqU2WL5ZFdOw6VhJ9c1aCpcM\nDWpqavCVr3wFd955J/bu3Yt3330XBw8exH/9138hlUrhq1/9Ku66664+5zQ1NeGnP/0p9u3bhwce\neIATLCHkFKhgyYhn/vz5uOuuu9DT04OlS5fiwgsvxI4dO5Cnro3T9cP48pe/DACYNm0ampqasmov\nIeSLca5pN2dLdqKI1a+ZSEffWopP80tToi4NQ46z2S0/qe2v0cBJK9dU1KH1J0+Fa1rB2h2WD9VS\ny5bEtXJZrf6vcj1Dr2dFBoejErXcG+qBxyMN3D1eyZlN+39NK1JZrpnQayaTcd2e7Nd9IYODqVOn\n4rXXXsO7776Lp556Cv/wD/8Ah+PMXw3rTQkhZOjR0tJzVsczD5aQc2TLli2oq6vD4sWLcc899+An\nP/nJ557zl7/8BQDwySefoLKyMtMmEkKGIFlSsJrfmhRfazyuStbyvdrFx2lX1eBySvSuw+GElblq\naHWkhKremC4Tut3QvFZ7Oi9WznNqGLLTKetJjTpOV36KaSRwRI2NRdTGCOJxGSMcFlXrVPsczs9G\nE+vnSvb2GYMMDSZMmID169fD6/XCbrdjxYoVeP/99894TkFBAb75zW+ioaEB3/nOdz53jNc3XnvW\nv5rJ0CEQ8PH5klOgD5aMeM4//3y88sorfbatXLky/e/a2loAwKZNmwBIUNTs2bOxatWq7BlJCBly\nZGWCtXsKAABujdB1m4UAAFPVp92h6lOVq92w8mTz4NL6xFZ930hE1GSitwsAEA+KnzSivlOH+kk9\nLglQcWqOrVVlyeGQqlJOlyha0y0q1eGU48NhuW4sFgWSVrSznOt2ye1ye/S2qd83kbAqPImvlj5Y\nQgghVLCEnCVr167NtQmEkCFAdhSsQ5Sdw2F109EdplVNSXNVLSWrPlm3xwuH/jthRW06RE2aut3Q\niGObCFgk1W+qJYkRT8p5RkJPVz+qFRFsU5+ty6O26fXcsRhMPdemsWBWYKmm5qb9vV67VpnS22mz\nsfA3IYSMdLIywSYTEjhkFdo37VZajqwb2sZOY6BgGFYAUyId+GTTRgGGvpb1uGSitRcUAQDcmp4T\n1lfIMZ1orXKGVtCTNWna7dZEaxml261GBB5HunGAkW5HpwFSVts9tc2ur7FtOtFaEy8hhJCRC9N0\nCCGEkAyQFQUbDkngkKUSrVfCVrK+qYrQVI2bLjThcMKhStVqiG63WcX8Zd3UAhRWkQqXwyrqL9e0\nApBSViqNld5jsxoKaIlFU/Yn4mE9PpluHGC3ZK5NX1Nb56SsV8Fyrs35GVVMCCFkxMKpgBBCCMkA\n2YkiVpUZSxeYsPyjGpCUTPQ53ErXcThdcMZFwSY92vLOaiFnRS3ptSw/byymDdp1TKuMrNWo3e1S\n/6jKzHhc/Kox9d1GIlIsIhwMIaH98xyqil1uZx/77A5NMwol9JJyDQY5EUIIoYIlhBBCMkBWFKwn\nT9J0rIhgS7FGo+K7NNSHmdLi+ab6ZhPxGGzqKzUTohoTGg2cSlgKVQvsIwQACEdl2dbSIetdUr7M\nXyDFmivHVgEAXHkS8etUFZp0qr81/NeG7iltFGCVdY8nUrquY0OVeNxqfSfqF+y3TgghIx4qWEII\nISQDZEXBujyas+oSdZjSqGFNH03nm0IVoQnNN00m0uUIrVxUq/1cyrQikWV7OCrnHthfDwDYvvXP\nMqYh6rh6vChXr/pgJ1RPAgBE1b+a0qUdBWpbMWLaVs/QkokpK4fWoY0EVJGbVsSytquz8n0JIYSM\nXKhgCSGEkAyQnShizXs11Jvp0NZyTo3GTTs5TY0QtmmBfhhQgZr2uaZU3cKlDdTjqhejIofzHH4A\nQKFLivrbtfD+mBJpMJCnKtqq2FTsKwMAhDXy1yhWU5I2pEzdlrQUtdVQXfN5VU0nrVZ6cauZO5tx\nE0LISIcKlhBCCMkA2Sn2r2mhlq8SqiotX2xS82GhUcRWjWK73QFDW9dZDQI0vRVR9blGQxK5a4+J\nepw3XdTvbH85AKD1Y4kidrR9AgAIde8FALT3jAcAVJw3HwDgq7xUxvZItLHdZgNMq4h/qo/dVhS0\nVefY8r0mYhJNHNMlGVps3rwZdXV1eOCBB9Lb7rvvPjz++OPweDynPScYDOKaa67BW2+9dcZrX3P/\na/2y4acPLuq/wYSQQQ3b1RFyBp5++ulcm0AIGaJkp5tOUn2TppW7qo3KVQmmrDY6qmCtMFwTznTU\nsFWRyTo3FVf/qHbNcceaAQDR40cAAKEWiSZuDklD9s5m2V/hEjU63i/b3V3aRD1QLQPkS3cew2HA\nZmitZPXFQusdO6E5uXE5NxbVTj0p+Rz2JN+8D1Xq6+tx++23o7GxEWvWrMGzzz6L119/HY888gic\nTic6Ozvx+OOPY+3atYhGo5gzZ06uTSaEDFI4ExDyKY4cOYJnn30WL774In7wgx/ANP9aNcTv9+OZ\nZ57Ba6+9hilTpuDll1/G9OnTc2gtIWQwkxUFa/kmDcPqYCPDOrUrTVLX00ox3QzdAZvVt9XQWsK6\n7vGpMzYmNYq79p8AAGx96wMAQPuJVgBAkV+ihKdNvgAA4LJLzeK4KuPeNu1VO7pNtrtHybrNDodN\n6xyrpE7/rdWllRebiIlyjWkd42iUPtihysUXXwyn04ni4mIUFBTg5MmT6X2zZs0CABw8eBBz584F\nAMybN29Axw8EfAN6PZI9+OzIZ6EPlpBPYRh/u0yIlVZmmma6hKfVcnGgaGnpGdDrkewQCPj47IYx\n5/rjKSsTrNORp//SqkyqZK0/Zna7+mY1qhg2S7W64XBK9KbDI/WMnU5L3eo11K8bbDoOAIgUVQIA\nkq4AAGD0BRIdfMGCrwIA8golT9Y0rBrGsoyk5Hrh3k4530zB1JrDlmRN/+lV5ZruxKOKNRIWv24s\nFunHXSGDkR07diCZTKKrqwvhcBhFRUWnHDNx4kTs2bMHS5cuRW1tbQ6sJIQMBeiDJeRTVFdX4557\n7sGaNWtw7733nlbRXnfdddixYwfWrFmDw4cP58BKQshQwDA/HcWRId556xVrOPmvRuMaliY0+vaD\nNVVY2+wu2B2qYN1Sz9ipXW8Mh76i098IJ/fvAQAcPXhAjtMKTb7iEgCAp1BrDBeIEq4cJbWJ/R5R\nKJ2tDQCAUG9H2lTT8r2qgrVZ0c2a92op1UhI+sCGg126XaKmb1z1v898Y8iIgq8Qhy98RTy8OddX\nxFSwhBBCSAbIig82v0AK/FqVm5xOiQi2a7RwIi5RuFZerEODSTx5eUhopx0rYtdmVVHSkk5xU34j\naDosQhHxh4ZDhwAAu/aIj6ytXXyrlWNEuc7/8mUAgKmTJM0ikdJOPtrixwYbnJYvWEtRWQoWeqzT\nLUrVqXWPXW6pd5zUz0MIIWTkQgVLCCGEZIDs5MGmHLrUvFJVox6XlfYgy1hUlF8oIr5MM5VAIi5K\nNd8rKjipKtiK/u1sEt/p4U92AwDC7e0AgCPHjgIAgqom7RqNXGiXpV+jkpMJ2Z+MWapTFbMdSKhv\nWAOX02HE6QpUutmmCjfPrXWTPc7+3BZCCCHDGCpYQgghJANkRcG2HZMqS/l+icRKOAwdXOb3onyJ\n8HXERK02qgq1J+0o9JcCAHxFEg1s09zUfTslWvgPv/8tAMDtFWX61SuuAgBMrm8CAHh84h/NLxT/\nqMMu6rJqzDgAQHGgAgDQ0y05rPhUJ5+UatSkXVRtPCo+127NlfVqfm/Kpoo1X9adLipYQggZ6VDB\nEkIIIRkgKwp2yozZAIDecAgAUFSqqtQvVZU8TjGjo60FAGC6RXWOHjMWDqf4SuMOUZGhHslT7Y6I\nuqzSGsOTLjgfAGCzKu+ExUdbNkGUaoEq2cOHJLq46aD0hy2PSQ6rzyfq2uWw1Gc8XS0qrr5jwy12\nusrFfqsIgVujjd3q53Vpzi4hhJCRCxUsIYQQkgGyomA/7pE6wU675L+mDFGljS0SLZxMac1fmyy7\n4+KDPXagCeGg+D3Dcc051ZBewy+/DdxOqdi09eOdAIB4RI5z20Vd7muoAwCEwr26X6ovefPEX2qr\nk+jjkG63OTTn1emAoQXde3rkXGu9wCc+47CeYxV8H1Uh/tzK0aMBAKuX3dLPO0QIIWS4kZUJ9sX/\n+yMAQL5HJrUSv7zG7e2SwKKoTlSmtpDr7tWAIxvg9shrV6s4RWmpBDs5dLKLhnSS0wApp0tez7bq\npO3U189ufW1rWL3mNP0nrhN4KiGvlMNaBjGaSCChQU3BNvkh0NEmE384KK+6HZqe43KJbc1V0mig\ncbRMtJxgCSFk5MJXxIQQQkgGyIqCDWrz86C+Sm1JSaBRStu92Yy+vTVDUVWlBmDXlBdLgXYea9Rz\n5BVwcbEUoMj3ymvnTut1rpZbjGs5xp5UVx+brKIWVou8lJnSpazH4jFYfRCSMavxumAFaVVXVwMA\nps+YIed6xUZPQX5/bgshhJBhDBUsIYQQkgGyomB72iS1xlKohhZwsFRnMilq0kp7yVM/qtvjgV19\nqEkt7m9TRWpoMFJIW8MFNQjK6r1XYJO0m65uaSFVX18v10los4CEXC8ZFRUd1+0pHccwbOmiEzZt\nkTd56hQAwJRp0wAAXg122ntoPwBg9y4JmOoJior+n//j1n7dH0IIIcMPKlhCCCEkA2Sn2L+qQ6tw\ng+Xn7I1INK5do3E9bm2u7hCz7E4H3Kpmw3Hxy3apOuzplWVMo3+jqmQtf2kqIWMYVk93laPW2Ib6\nfU1dQrc7VCGbyRRiVqMAU+xp1xKOb9TUiA1BKVJh+YehLfOSCctbS4YKwWAQ11xzDd56662MXP+a\n+1/DTx9clJFrE0IGJ1SwhBBCSAbIioKNa7F8FapQVyyS2vbNruoxbopKtRqxp4JBJOJWg3VVptax\nGoFs6sUs5Wo1aE+3tdOlpVBNbZWXsAr5axd1A2qc7k+l4mklmpLa/uhoE9Wcl+dS+2V/WO1Oqko2\n7PzdMhTo7e3F2rVrEY1GMWfOHADA9u3b8dRTT8HhcGD06NF45JFH4HK58PTTT2P79u1IJpNYtWoV\nvva1r+HBBx+E0+lEZ2cnnnnmmRx/GkLIYIMzARmxvPbaa5gyZQpefvllTJ8+HQDwb//2b3j22Wfx\n4osvorS0FL/73e+wfft2NDQ04KWXXsKLL76IH/3oR4hocRS/38/JlRByWrKiYPfVSfPzz/pBrahi\na5a3qi7ZVFXaDFtaucZVTZqWStRz7NaxKlSt6GC7qki7OmG1SBTsWgEqmVaqul99sVaxf4/DDZtL\n9sXs6lw15Byr/7qmziLxGX8vUvTBDgUOHjyIuXPnAgDmzZuH1tZWdHR0YO3atQCAUCiE4uJiNDY2\nYufOnVi9ejUA+f+2pUUaU8yaNavf4wUCvgH+BGQwwedLPktWJlhCBiOmacJm+2uRE6fTibKyMmza\ntKnPcT/72c9www034B//8R9PuYbT2f/evy0tPV/MYDJoCQR8fL7DmHP98ZSdPNiu2Gm3W0o1T9vA\nWVLWUoYOtxMeV57uk2MtJRuzCu1bfly7XKOwUBusu8VPqmI5rZatCGWvNkcvKZVKUCXFUh+5SBuz\nu13OdNRzV1C+ON093X3GammWqOJ6be7e3iXO2rj6g8ngZuLEidizZw+WLl2K2tpa+LV94oEDBzB5\n8mRs2rQJc+fOxaxZs/Dkk0/i9ttvRzwex5NPPol169bl2HpCyGCHCpaMWK677jrceeedWLNmTTrI\n6dFHH8VDDz0Ep9OJ8vJyLF++HC6XC5deeimWL18O0zSxcuXKsx7r9Y3XUuEQMsIwTKvgbgZxODTn\nVNetEd2qMj1OUadJzWl1qsrML8hPd6qxInajMa36pNcq1Ebpfp8oT0OPM9UPavl78/K96WsCgGlF\nD+vYXq+M49J2eHbDjog2bQ+HJMfWZhMV7HLLa8GYdvBpaxXl2nhSlGwwJPm9jW2hftwdMlLgBDt8\n4Svi4c25viJmFDEhhBCSAbKiYAkhhJCRBhUsIYQQkgE4wRJCCCEZgBMsIYQQkgE4wRJCCCEZgBMs\nIYQQkgE4wRJCe4aJOAAAHhBJREFUCCEZgBMsIQPIY489huXLl+Omm27Crl27+ux77733cMMNN2D5\n8uX44Q9/mCMLyRfhTM930aJFWLlyJVavXo3Vq1ejqakpR1aSL8L+/fuxePFi/PznPz9l31l/h01C\nyIBQW1tr3nHHHaZpmuaBAwfMb3zjG332L1u2zDxx4oSZTCbNFStWmHV1dbkwk5wjn/d8r7jiCrO3\ntzcXppEBIhgMmqtWrTK/+93vmps2bTpl/9l+h6lgCRkgtm7disWLFwMAJk2ahK6uLvT29gIAjh8/\nDr/fj9GjR8Nms2HhwoXYunVrLs0lZ8mZni8ZHrhcLjz33HMoLy8/Zd+5fIc5wRIyQLS2tqK4uDi9\nXlJSku4b29LSgpKSktPuI0ODMz1fi/Xr12PFihXYsGEDTBbJG3I4HA54PJ7T7juX7zAnWEIyBP/A\nDm8++3zvvvtuPPTQQ9i0aRPq6upQU1OTI8vIYIETLCEDRHl5OVpbW9Przc3NCAQCp93X1NR02tdQ\nZPBypucLSPvD0tJSOBwOLFiwAPv378+FmSRDnMt3mBMsIQPE/Pnz06pl7969KC8vR0FBAQCgqqoK\nvb29qK+vRyKRwNtvv4358+fn0lxylpzp+fb09OC2225DTNtpbtu2DVOmTMmZrWTgOZfvMLvpEDKA\nbNiwAdu3b4dhGFi/fj0++ugj+Hw+LFmyBNu2bcOGDRsAAFdeeSVuu+22HFtLzpYzPd8XXngBr776\nKtxuN2bMmIF169bBMIzPvygZNOzZswdPPPEEGhoa4HA4UFFRgUWLFqGqquqcvsOcYAkhhJAMwFfE\nhBBCSAbgBEsIIYRkAE6whBBCSAYY0hPst771LSxcuBB/+tOfcm0KIYQQ0ochHeQ0ffp01NTUYNy4\ncbk2hRBCCOnDkFWwq1evRiqVwm233YYbbrgBTz/9NJYtW4YPPvgAnZ2duOeee7B06VJcffXV+PGP\nf5w+b/PmzZg/fz6+/vWvY/PmzZg2bVoOPwUhhJDhypCdYDdt2pRe5uXlYc+ePdiyZQsuvvhiPPXU\nU/D7/aipqcHLL7+MX/ziF9i+fTs6Ozvxr//6r3j++efx6quv4p133snxpyCEEDJcGbIT7GdZuHAh\nbDb5OH/84x+xcuVKAEBRURGWLFmCd999Fzt37sSECRMwdepU2Gw2rFixIpcmE0IIGcYMmwnW7/en\n/93e3o7CwsL0emFhIdra2tDd3d3nuIqKiqzaSAghZOQwbCbYT1NWVobOzs70emdnJ8rKylBQUIBQ\nKJTe3tzcnAvzCCGEjACG5QR7+eWX41e/+hUAUbNvvPEGLr/8cpx//vnYt28fjh49ilQqhVdeeSXH\nlhJCCBmuDMsJ9t5770V3dzeuuuoqrFq1CnfccQdmzZqF8vJy/PM//zNuueUW3HjjjZgzZ06uTSWE\nEDJMGdJ5sOeCaZrpDhd1dXVYuXIltm3blmOrCCGEDDeGpYL9WyQSCVx22WXYuXMnAOC3v/0tZs+e\nnWOrCCGEDEdGnIJ94403sHHjRpimiUAggEcffRTjx4/PtVmEEEKGGSNugiWEEEKywYh6RUwIIYRk\nC06whBBCSAZwZGOQm677OgBg+nnnAQDcbg8AwFfoAwCEQ2EAQFtbCwDgZMNhAIDdbk+XP3QY+QCA\nseOqAQCllaMAAMeOHgIAhDqkaETKjMqgRhIAENFrO+xuHUu2n2zR4yHrRd4CAEB5QKo77fpkH7p6\neuRShtgQS8YBAHm2hJybSgEAWrqleIXTkLftyUgMAHC0M9jPO0SGO4lEEh0doc8/kAxqiou9fI7D\nhLN5loGA75zGoIIlJAs4HPZcm0AGAD7H4UM2nmVWFOyoMRKlO2b8JABA08kGAEBXVxcAYPTo0QAA\nu0Pm+0OHDwAAotEQqqqqAABOwwkAaO8UlesqFEXqdKhqjPbKMhXVpajNnpCozc7OdgBAc5MsA4FS\nAMC40aKEA0XyC6W7S/YX5ztRUjwGANAblms5ExEAgM/rAgAYbrFpXEy2p2Iyts+TdxZ3hxBCyHAk\nKxMsIYOdeDyOlStXorq6Gk888cSAX/+a+18b8GsSQs6d1zdem/ExsjLBvlu7HQBQOkoUYTQmivCj\nXR/2Oc7lEmXY3iVqNBaPozAk6rC8SPy2He3iO+0IdQAAfAWiPJM29bFG5fiOTlHHDY2iSMNh8bVO\nniQN1idOnAIAKCwUtZnnFKU7Zqwo2nFVXbB7iwEAW3d8BAAIOEWxdgflvX17RPy7TpvR53MVes/t\nfT3JHS0tLYjFYhmZXAkhIxMqWEIAPP744zh27BgeeughJJNJ1NfXY9OmTdi4cSM++OADJJNJ3Hzz\nzbjuuuvw3nvv4bHHHkNZWRkmTpyIkpISrF27NtcfgRAyyMjKBNtQfxQA8OabbwAAplZPAAD4i6Q3\na329+GTDEVGfLo+oVbfbA9MQR3ShT6J8Qz2iTHfs2QsA8BZI31ebHheNyzU6u8RXaybER3vJ3LkA\ngMmTpwMA7C65Xp5TzkvFRBG7CuSWeAwDJ9q6AQA+t6jjiePEl9yqkcjOsCjt7lb5fJGIbG/pDvf/\n5pBBwQMPPICGhgZUVlbi0KFDePnll7Ft2zbU1dXhl7/8JUKhEL7+9a9j8eLF2LBhA5588klMmzYN\nN998M+bPn59r8wkh58C5Rgf3FypYQj7DrFmzAAB79uzBXP1h5vV6MXnyZBw9ehQNDQ2YMWMGAGDB\nggVIJpM5s5UQcu60tPT067hznYizMsHOnjIWANDU1gQA+H97pdh+cYko2IoKyT11OMWcWFDyRz1F\nRUjF5I9XUtVtTBumxyOSgxo0Zb1V81oTcVGshUUy9tiScgBAkUuunWeT65mmqE97j5zvhVy/JynH\nhbpDcNjFt3rJBIly7o6Iem4PyjUMU3zGXpcOVuyVa9r5u2Uo41Rfu9V1ySIej6fzsi0+ewwhhFgw\nD5aQv8HMmTNRW1sLAAgGgzh27BjGjx+PQCCAgwcPIplM4t13382xlYSQwUpWpJZHKzcFSmS4SERU\n49FjxwD8VTF41NfptIlf1Eil0NEs/tmPekShBlXBJrViU6RL1K7XkGtXj60EAIwukcpPHq+oylKX\nRAsXm6J8QyE5r9QnY7nU1t7uTvmHDfB65ZpjfGJX9zGp0GT5iE2H+H8TWumpyCvHlZcH+ntryCDm\nkksuwcyZM3HzzTcjkUjg/vvvh9frxb333ou1a9eiqqoK1dXVp6ja0/H6xmv7/TqKDF4CAR+fI+k3\nfJdJCICqqips3rz5lO333XffKds8Hg9+/OMfo6qqCt/73vcwbty4bJhICBliZGWCjcbFZ2nXur8V\nWkXJpj5Xv198sR0dEsnrtCKH89wIFIkSTWpXvbhd9lVXim81pdcOFMg1RheIYi2wiSqOaA1in+bY\nerWecKhHlWqeXD+vUI4vjcsy2tKCzrD4ZSu84oOtGitjJtpEyQbVB+sPSIWqomJxhFuKnAxPTNPE\nXXfdhfz8fJSWlmLp0qW5NokQMgihgiXkLLnssstw2WWX5doMQsggJysTrF0VXb4uU0HJE60cI5Wd\nrJ7vZRp9G4+KQvS47KgsFT9nNCLH5CXF31WcL9fyumRZ4JPlqApRsF0dMoY2xEEiJZWaCjXP1e4o\nAQA0drUBABwFUrWp0COKtrzKhg+Oi4/4w7oTAICLZso51SViw+F28eOGIOd0W/7ivZKje//a/9XP\nO0QIIWS4kZUJ1rTyBJ0yMZUXyaTZ2SOTYCguE2qhFo0w8mSS9DriCIfkVa4ZkVfDxRpgNLpEUmOq\nJ0ozgOJRMsFWjpdrvPMnmeQcOjF7tB1dxJTjEtokIKI2Gh55vRuJS/pOgd2GBbNnAwB2HJZCEkcP\n7wcATBgjY5ohOfbNmj8BADp6ZT3cwyAIQggZ6TBNhxBCCMkAWVGwnV2iQn1eSV+pDEhBfXtcCk+4\nbGKG067N1W2iaKdMHI1LLpwKANj9l90AALdLFOz06RMBAB6PKFKHQ9SwyylqufmEvN5FWF7fuou0\nhZxNlKzV1q6jXV7/Fuv+UaMkkOnwoQYsnCDRoVdeOhMA8Kc/7wIAnGgSuwu9ElhVli/BTq29oopj\n8Xi/7w0hhJDhCRUsIYQQkgGyomC12iFMU/yo7e1SRN+m5R3yXTrPq6ocUyYqdeGlsxCPiz9z3CQJ\nQpoyVVJi8lQ1djS3yjU0gKqjSYr8l6kiLZkgZRjLy+V8p5ZMLC0XZVymAUtHj4qf1WWIKrX7CvC7\nP4pvddnCv1N7vgQAePOdtwEA7jz5YAu+dAEAIPxnUdmNqVS/7w0hhJDhCRUsIYQQkgGyU+xflV+s\nRdRmRMsdxqKyLPSJGVMnSXTu7JmiUptaTmLbh9Ksffr5ojihCtSVLyq3dLT4TLtbxM9r15SZv1v0\nVTnO5VYrxD/aq5G+CYiftHyU+IULiySK2GqaPm7sRNQel7SbP7z7PgDgqq9eAQC44itzAAD7DtQB\nABoaTwIA4j3S3N1pN/t7awghhAxTqGAJIYSQDJAVBevWvNZQQnyWhVo8310sSu/8qVKgf+b0KQCA\nsLacc/n8uOgrl8uxGml8/Jgo1RMnpKxiXp5cKxmX3wo2LVZhFZCwaWlFmxbkd2rJRI9HbGnXpupB\nLf5vJsR/mgp3oFTb6EV65Zi33v4DAGDBwosBAPPmXSjnuI4AAN7bdRgA0NzS1d9bQwghZJhCBUsI\nIYRkgKwo2KYDUlWpq03UZ8k48ZsumlsNAJhzofhc3cWiZE80i/o0WkLwJ0T9treLvzaUknxXj198\nsDapgIhkRP4RDIratKty9frEt+orkuhgq7VYXAv5p6Kilu02UcIOLaXYFW2APU+Obe4Wtbv34yMA\ngM6wqN1LFlwCACgIaMN4Vc+mNisghBAycqGCJYQQQjJAVhRsvkb+xrySq3qyU9To7nqJ6O0wjwAA\nJs2UCk+BElkWhINwRUUN+ieIAq2aIWrX4RLladok6renUyKUw53q/1Rfql2boLu0mXs4Igq4t11U\naSIktp3Yd1Bs6ZJIYKPADmeR1DuuOE/GnHyhVHRKdshYP/nlKzK21kmOhOWa+Xmeft8bMrTZtm0b\nqqurUVpammtTCCGDDCpYQr4Av/nNb9DW1pZrMwghg5CsKNi9B44AACpKRIWGekVl7vhI/Kbe4osA\nAO3bpdbv4eMSjXvR1In40szzAACNJ44DAPLatX1dWHytpk1Uo7+oWLeLsvV7pOaw0y0+3I5uGdOm\n0cglAfEDG6asX+SSakztJxoBAF09QdSrL3jbTqnc1BSSSlOzps0AACRS4nutPyz2ztW6yXlNvf2/\nOWRQEo/H8eCDD6KhoQFutxuPPfYYHn74YYRCIUQiEaxbtw49PT148803UVdXh2eeeQaVlZW5NpsQ\nMohgw3VCTsOrr76KsrIybNy4EVu2bMGbb76JG2+8EYsXL8bWrVvx3HPP4ZlnnsH06dOxbt06Tq6E\nkFPIygT7UZ0oPM90iRqeViWN1seMlT9KqZgo2dEB8V32lEh1pV+9+jZqP/wIAFBWJPu+ZIjJbY31\nAIDudolMDofE59reIetur9YiLpUm6Slt6u7Nk+2lZaJ4d+78AAAQCYlv1q99Yz0JG/xa3anaJ/61\nREyUabJLuv0EHOLXzZ84QWzU/N5Egm/ehzp79+7Fl7/8ZQDA3//936OnpwcPP/wwfvKTnyAWi8Hr\n9Z71NQMB30CbSXIAn+PwIdPPkgqWkNNgt9uR+lTThhdeeAEVFRX4j//4D+zevRtPPvnkWV+zpaVn\nIE0kOSAQ8PE5DhPO5lme60SclQk2oP1foxHxYcYh0cT7GkRtNu3+BABQ4hFzFl4+HwDg8vrw3x9K\nDm15gajD8mJRvzOnTAMAmEWiKs2UqMpDLdLftalT/KfloyUSuFuVbVvbSV3K/o4uiUJu7ZXI5gPt\ncsODoS6MC4jKnT1VlPeV0yerLWJ/XlI+j0cV7ns7pTbxoYb6s7g7ZDBywQUX4P3338eyZcvw9ttv\n40c/+hHWr18PAHjzzTcR156/hmEgmUzm0lRCyCCF7zIJOQ1XX301wuEwVq1ahRdeeAHPP/88nn/+\nedx6662YNWsWWlpa8Jvf/Abz5s3D3Xffjbq6ulybTAgZZBimaWa89cvd3/ym/CMmFZB6e8TfeaTh\nmKyHJMI3JcWVUDmmDAAwfkIVxk4cDQBoOiKdbfb8ZQcAYM4FEsl7geaoHjoqf+Dc6jf90gXnAwDy\nvFrDuF6UbXurKN6tf5GI5Z64qA+r+lJJuYx94UVTUeyWba6YqNuwVqKaMEa6/qTiYnBM7+Cu46KK\nt/xpDwDgjdrd/bo/ZGTAV4tDH74iHj5k4xUxFSwhhBCSAbLig41AooQL/fIrwJ0SyVful4hdIya/\nIlqjogh37BCfbENDEy4KSX7ql+ZK55rq8eKD/XiXRP8eVn/nkROiLnuPiI+1tUlU59hK8cH6tRZx\nNCmqtHqK+FN7VZ1WlEvkcmmJ+F1DXV3w2MVeR1J8ru4SGfuTY1pYQDv0jKoUH7PNIbm5NqP/94YQ\nQsjwhAqWEEIIyQBZUbD1J0VVBopEsQZbpZdriUOG95VZVZW0jrB2pak/3oyTJ/4bAPDJxxJNfOlc\n8b1Oni5RxEc1x3aMqsjmjhYAwJ93i0/2kwPS/9VuaKSnqsuUrpeUiE1lhZLXaERErTriCdhTonYr\nRk2QsRo0AjkoEctlleMBAC1aPcqudrtsGXdrE0IIGeRQwRJCCCEZICsKNhUS32pHWPydSe1oM0lV\naL5d5vmESyJ9PX6J9PUXFqCpUdTugUOy79AhUaxjR4nPdOGlswEA1141BwCw/4D4b+uOStTx0ZMy\nlk0Va4FPFG2wV8ZoahTf7YWTxJZ4t9hYXjka1ZOnAwDq60UVW514Rgck7/WjA2LLxfNk7OaTsu7P\nd5/F3SGEEDIcoYIlhBBCMkBWFGzD8SYAQLRX1OKM8ySC16MdcJIaPVxRIcowZpOo41BvDy6eI/ms\nwR5RnCe1q47TLb7S/UdF2e7+RHyul8yWvNgxb38IAGg+KWMaGvHb0y3+U7tdlOyMieK79XjEB+vK\nk+2jJk/F/gYZ66Ndks86aZTk5B48ImMWF0tkcmWZdPh57eNDAIDWHlb2IYSQkQ4VLCGEEJIBsqJg\nmzokOthlSHRtfaP4NPefFGVbohG81RMnAgDGjZfo3PoT9SjW/NXiYlGJjRqRrJdC40nJg21pFv9o\ntEd8qJfOEJVs2GTdkZLfEjHIiQ63LBdeIBHMyTxRxBOniN+1vaUde3buFPv8MnZzp1Si+uATUar/\n69abAABtJ2V9/1H5PO7C0f2/OYQQQoYlWZlgCwvzAQBlmqbj11Zyzc3NckBCJtGWE/LqtXrsBADA\nhPISXPp3swAAUVNeI8+bI5OvNyH5NkZcXvl2NLfodpkEywtlDJ9POqIUuqVoREyLtMch5xXq5Fk6\nbgoAIKmBWPve+2+MLZd2ehFtyv7qG78DAFSOkQnUejX8yh/eAAAEI3JtX4m93/eGEELI8ISviAkh\nhJAMkBUFW1wiSs/jkvk8mRSl19UVViMkqCkWFPUZ6pHlpV+ahQMH5PWrwyOvdM2IKNlCQ4KREJeW\ncR1tco7NlPVDTZLe0x2U9SK3qOjSfHkdHdPzE6poDx2ScUJa0L/AW4AefbVd+6EEORVpis/XrpRG\n3CdPSCrQ+7sPAgBaO6XkY2/vvv7fHEIIIcMSKlhCCCEkA2RFweYXiD8UMVGsvUFJucl3SkGG1tZW\nAECFFnAoDkgRicauEN58530AwKgiUZ6mtohraRGfq8MpvxHMhIxhs8lHataxnG45r8TpAQAU+EsA\nAIZNUml6ekX5HjgprfN8bvET20tL0FovaTrV4ysAABfNlQCoEp+M+ft3/gIA2HNM1bIW1EjYov2/\nOYQQQoYlVLCEKJs3b8YTTzzRZ9t9992HiLolTkcwGMSiRYsybRohZAiSFQUbD0nEboFTIn9LyqTA\nhNMu0ba9PdLmzbDL/nBEFGDdnoPIc4jvtKtL/si53aJ6o6Yo1tZOUcN2u6TZGNpAwJYnSnXcKGmg\n7vPJdVJaxKKjoxEA4C+WNJ05sy8CAIQisj/pdGKcR+yZUCoquLlRlPbmP0irvN9vE99rMK7t7Lyi\nfs0UC00MF55++ulcm0AIGaJkZYIlZKhQX1+P22+/HY2NjVizZg2effZZvP7663jkkUfgdDrR2dmJ\nxx9/HGvXrkU0GsWcOXNybTIhZJCSlQnW6xa1OXmsqMkir6jQWFSUbaxUfK9WEYl4THJXnXYXJo6X\nnNO2JonYPdEoxRyKtcyiO1/yWwt8kktbpLmrnVpwosAnYzt0madjTxmrLfIs1ayN2EOaV+t22dAT\nkcjlX//uXQDA7r2iWA82iM81Ysi1SgNSbhFJVdexWP9vDhlUHDlyBJs3b0Zvby+uvfZa2O1/zWn2\n+/145JFH8NJLL2HKlCn4zne+g9/+9rfYsmVLDi0mhAxWqGAJ+RQXX3wxnE4niouLUVBQgJP6ow8A\nZs2SoicHDx7E3LlzAQDz5s3r97UDAd/AGktyAp/j8CHTzzIrE+z4MaLwpk+VKkz5LlF6ZlLU4wmt\n4NTeIsowoXmxtlgPjh/VRgBaaD+vR6KDwzE5BoYq0Lio3m5tit6oTd09qliL/eJHdWsz9FRU8l+D\nXZK7GgqLH/jCC6W5gA3Alt//AQDQpCUSI6aM5XBIPmyBV5Zuu/hc3S4ZI6WKnQw9DP3/6XQ4neJr\nN00TNpvEB6ZSqX5fu6Wl54sZR3JOIODjcxwmnM2zPNeJmFHEhHyKHTt2IJlMor29HeFwGEVFRacc\nM3HiROzZswcAUFtbm20TCSFDhKwo2CkTxwEAKsvFB9vWJuqyvqkdAHD4uOSbxtXf5fOIMgz4XDh6\nQvJdP94vKjKeUP+sqgm//gG0FGxQ/bqeAvHJevMlejiW0BrEKYlQjnWKjzamDdbHTxTfrUtdbjU1\nf0RCRXJpifiI24NybZ9PooW9HrGhvEzydlMJUcehULjf94YMLqqrq3HPPffg6NGjuPfee/Gf//mf\npxxz3XXX4c4778SaNWsY5EQI+ZvQB0uIcv311+P666/vs+3aa68FAPz7v/97elthYSE2bdqUXr/7\n7ruzYyAhZEiRlQk2zytVlD76+GMAwAd7ZHm0VWr92iDqs0Rb03m16tKoQDHs6q/9YJ+o3INHjuk1\nRZlO0GpQXo0mNrTLTlJdY9GQKNeCPDnO45C34lMmiOq0JUWV5vtFujacPAIAOG/qZMT0WkePyZgF\nfvGtJmyiXKurpbJTfqGMvfcTeW3YGw7299YQQggZptAHSwghhGQAwzRNM9dGEEIIIcMNKlhCCCEk\nA3CCJYQQQjIAJ1hCCCEkA3CCJYQQQjIAJ1hCCCEkA3CCJYQQQjIAJ1hCBpDHHnsMy5cvx0033YRd\nu3b12ffee+/hhhtuwPLly/HDH/4wRxaS/nKmZ7lo0SKsXLkSq1evxurVq9HU1JQjK8nnsX//fixe\nvBg///nPT9mX8e+kSQgZEGpra8077rjDNE3TPHDggPmNb3yjz/5ly5aZJ06cMJPJpLlixQqzrq4u\nF2aSfvB5z/KKK64we3t7c2EaOQuCwaC5atUq87vf/a65adOmU/Zn+jtJBUvIALF161YsXrwYADBp\n0iR0dXWht7cXAHD8+HH4/X6MHj0aNpsNCxcuxNatW3NpLjkDZ3qWZOjgcrnw3HPPoby8/JR92fhO\ncoIlZIBobW1FcXFxer2kpAQtLdINqqWlBSUlJafdRwYfZ3qWFuvXr8eKFSuwYcMGmCyINyhxOBzw\neDyn3ZeN7yQnWEIyBP/oDh8++yzvvvtuPPTQQ9i0aRPq6upQU1OTI8vIYIYTLCEDRHl5OVpbW9Pr\nzc3NCAQCp93X1NR02tdWZHBwpmcJSE/g0tJSOBwOLFiwAPv378+FmeQLkI3vJCdYQgaI+fPnp5XM\n3r17UV5ejoKCAgBAVVUVent7UV9fj0Qigbfffhvz58/PpbnkDJzpWfb09OC2225DLCatLrdt24Yp\nU6bkzFZybmTjO8luOoQMIBs2bMD27dthGAbWr1+Pjz76CD6fD0uWLMG2bduwYcMGAMCVV16J2267\nLcfWkjNxpmf5wgsv4NVXX4Xb7caMGTOwbt06GIaRa5PJZ9izZw+eeOIJNDQ0wOFwoKKiAosWLUJV\nVVVWvpOcYAkhhJAMwFfEhBBCSAbgBEsIIYRkAE6whBBCSAbgBEsIIYRkAE6whBBCSAbgBEsIIYRk\nAE6whBBCSAbgBEsIIYRkgP8PXHkXjQ0BJu4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f9875ac6eb8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}